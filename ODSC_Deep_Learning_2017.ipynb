{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# CSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".expo {\n",
       "  line-height: 150%;\n",
       "}\n",
       "\n",
       ".visual {\n",
       "  width: 400px;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "style = \"\"\"\n",
    "<style>\n",
    ".expo {\n",
    "  line-height: 150%;\n",
    "}\n",
    "\n",
    ".visual {\n",
    "  width: 400px;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\"\"\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_all = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning from Scratch using Python \n",
    "\n",
    "Seth Weidman\n",
    "\n",
    "11/30/2017\n",
    "\n",
    "https://github.com/SethHWeidman/ODSC_Neural_Nets_11-04-17/blob/master/ODSC_Deep_Learning_2017.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# To begin..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nah..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "This talk will have two parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Part 1: Neural Nets from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We'll implement a basic neural net with one hidden layer, from scratch, and use mathematical principles to get the backpropagation right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Part 2: Transitioning to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We'll transition to Deep Learning by changing our mental model of neural nets to be that they contain \"layers\" which pass information backwards and forwards between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We'll show how this framework can be used to construct arbitrarily deep neural networks, and show how these can learn MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: Neural Nets from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "We've all seen diagrams like the following in the context of neural nets:\n",
    "</div>\n",
    "\n",
    "<img src=\"img/neural_net_basic.png\" class=\"visual\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Part 1: Neural Nets from Scratch\n",
    "\n",
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_v3.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "Many don't fully understand what is going on in this diagram. This talk will attempt to rectify that.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why neural nets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "Let's suppose we need a function that  that can learn a complicated relationship between inputs and outputs:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df, X, Y = generate_x_y()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why Neural Nets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1  X2  X3  y\n",
       "0   1   0   0  1\n",
       "1   0   1   0  0\n",
       "2   1   1   0  1\n",
       "3   0   0   0  0\n",
       "4   1   0   1  0\n",
       "5   0   0   1  0\n",
       "6   1   1   1  1\n",
       "7   0   1   1  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "So: we have eight observations, and a complex relationship between inputs and outputs. Now, let's build a neural net that can \"learn\" this relationship.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why Neural Nets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "How to even begin? Well, let's look at this from as high a level as possible and then progressively dive deeper. First, we want a function $N$ that--based on the data from before--maps inputs to outputs properly, that is:\n",
    "</div>\n",
    "\n",
    "$$ N(1, 0, 0) = 1 $$\n",
    "$$ N(0, 1, 0) = 1 $$\n",
    "$$ N(1, 1, 0) = 0 $$\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "First, let's observe that logistic regression can't do this. That is, there are no parameters $b$, $w_1$, $w_2$, and $w_3$ such that:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$N(x_1, x_2, x_3) = \\frac{1}{1 + e^{b + w_1 * x_1 + w_2 * x_2 + w_3 * x_3}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "This is true for the same reason that logistic regression cannot learn XOR. Our problem is a three dimensional problem since we have three features, but you can easily see in two dimensions that the space is not linearly separable:\n",
    "</div>\n",
    "\n",
    "<div class=\"visual\">\n",
    "  <img src=\"img/xor.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "Of course, we *could* manually do feature engineering...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... but who likes feature engineering???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "Let's have the computer do the feature engineering for us, via a neural net learning hidden features!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's make a prediction using a neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our goal will be to:\n",
    "\n",
    "* Start with our three original features.\n",
    "* Transform them into four \"intermediate features\" using logistic-regression-like transformation.\n",
    "* Take these four \"intermediate features\" and use _them_ to predict our final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do we transform our original three features into an intermediate, or hidden layer? Let's call our intermediate features $a_1$, $a_2$, $a_3$, and $a_4$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want $a_1$, for example, to be a linear combination of $x_1$, $x_2$, and $x_3$ - that is, we want some weights $v_{11}$, $v_{12}$, and $v_{13}$ so that:\n",
    "\n",
    "$$ a_1 = x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} $$\n",
    "\n",
    "and similarly for $a_2$, $a_3$, and $a_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A way to concisely express this is to define your features as a vector\n",
    "\n",
    "$$ X = \\begin{bmatrix}x_1 & x_2 & x_3\\end{bmatrix} $$\n",
    "\n",
    "This should be intuitive, since $X$ is already a row in your data! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then, multiplying this vector by a matrix $V$:\n",
    "\n",
    "$$ V = \\begin{bmatrix}v_{11} & v_{12} & v_{13} & v_{14} \\\\\n",
    "                      v_{21} & v_{22} & v_{23} & v_{24} \\\\\n",
    "                      v_{31} & v_{32} & v_{33} & v_{34}\n",
    "                      \\end{bmatrix} $$\n",
    "\n",
    "gives us what we want, since \"$A = X * V$\" is equivalent to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ a_1 = x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} $$\n",
    "$$ a_2 = x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} $$\n",
    "$$ a_3 = x_1 * v_{13} + x_2 * v_{23} + x_3 * v_{33} $$\n",
    "$$ a_4 = x_1 * v_{14} + x_2 * v_{24} + x_3 * v_{34} $$\n",
    "\n",
    "which is what we want in order to get four intermediate features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's code this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[1 0 0]]\n",
      "The dimensions are 1 row and 3 columns\n"
     ]
    }
   ],
   "source": [
    "x = np.array(X[0], ndmin=2)\n",
    "array_print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.51  0.59 -1.69 -0.82]\n",
      " [ 0.65 -1.18  2.04  0.29]\n",
      " [-1.31 -0.18  0.07 -0.2 ]]\n",
      "The dimensions are 3 rows and 4 columns\n"
     ]
    }
   ],
   "source": [
    "V = np.random.randn(3, 4)\n",
    "array_print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.51  0.59 -1.69 -0.82]]\n",
      "The dimensions are 1 row and 4 columns\n"
     ]
    }
   ],
   "source": [
    "A = np.dot(x, V)\n",
    "array_print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we?\n",
    "\n",
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_first_layer.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 2: feeding these intermediate features through an \"activation function\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We're going to use a classic, easy-to-understand activation function, though one that is not often used in cutting-edge applications: the sigmoid function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 2: feeding these intermediate features through an \"activation function\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ B = \\sigma(A) $$ or\n",
    "\n",
    "$$ b_1 = \\sigma(a_1) $$\n",
    "$$ b_2 = \\sigma(a_2) $$\n",
    "$$ b_3 = \\sigma(a_3) $$\n",
    "$$ b_4 = \\sigma(a_4) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 2: feeding these intermediate features through an \"activation function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.62  0.64  0.16  0.31]]\n",
      "The dimensions are 1 row and 4 columns\n"
     ]
    }
   ],
   "source": [
    "B = sigmoid(A)\n",
    "array_print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we?\n",
    "\n",
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_first_sigmoid.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3: use these intermediate features as a linear combination to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll multiply these \"sigmoided\" results by another matrix $W$ to get a single output. Since we want to transform 4 features down into 1, we can use a 4 x 1 matrix:\n",
    "\n",
    "$$ W = \\begin{bmatrix}w_{11} \\\\\n",
    "                      w_{21} \\\\\n",
    "                      w_{31} \\\\\n",
    "                      w_{41}\n",
    "                      \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3: use these intermediate features as a linear combination to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And since we want the result to be:\n",
    "\n",
    "$$ c_1 = w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3: use these intermediate features as a linear combination to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is equivalent to writing:\n",
    "\n",
    "$$ C = B * W $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "or:\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "c_1 \\end{bmatrix} = \n",
    "\\begin{bmatrix}b_1 &\n",
    "                  b_2 &\n",
    "                  b_3 &\n",
    "                  b_4\n",
    "                  \\end{bmatrix} * \n",
    "\\begin{bmatrix}w_{11} \\\\\n",
    "               w_{21} \\\\\n",
    "               w_{31} \\\\\n",
    "               w_{41}\n",
    "               \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3: use these intermediate features as a linear combination to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So we can simply code this up as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 1.3 ]\n",
      " [ 0.32]\n",
      " [-0.79]\n",
      " [-0.15]]\n",
      "The dimensions are 4 rows and 1 column\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(4, 1)\n",
    "array_print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.85]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "C = np.dot(B, W)\n",
    "array_print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_second_layer.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 4: sigmoid this to make a final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically, we want:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ p_1 = \\sigma(c_1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So we can simply code this up as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.7]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "P = sigmoid(C)\n",
    "array_print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_final_prediction.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 5: compute the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically, we'll compute mean squared error loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ L = \\frac{1}{2}(y - P)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And coding this up is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.04]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "y = np.array(Y[0], ndmin=2)\n",
    "L = 0.5 * (y - P) ** 2\n",
    "array_print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_loss.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have made our prediction and computed our loss, $L$. Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall: each \"step\" is just a function applied to some input that results in some output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we write out what we just did in terms of mathematical functions, we could write it as:\n",
    "\n",
    "\\begin{align}\n",
    "A &= a(x, V) \\\\\n",
    "B &= b(A) \\\\\n",
    "C &= c(B, W) \\\\\n",
    "P &= p(C) \\\\\n",
    "L &= l(P)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So, say we have a neural net with just one hidden layer. We could write the loss of a neural net on a given observation $ x $ as:\n",
    "\n",
    "$$ L = l(p(c(b(a(x, V)), W))) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically, we _want_ to change the weights in such a way that the loss will be reduced during the next iteration. The equations:\n",
    "\n",
    "$$ W = W - \\frac{\\partial l}{\\partial W}$$\n",
    "\n",
    "$$ V = V - \\frac{\\partial l}{\\partial V}$$\n",
    "\n",
    "do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice that this \"makes sense\":\n",
    "\n",
    "* If $\\frac{\\partial l}{\\partial W}$ is a positive number, then we want to _decrease_ the weight, since increasing the weight would _increase_ our loss. That is exactly what the equation $ W = W - \\frac{\\partial l}{\\partial W}$ does.\n",
    "* Similarly, if $\\frac{\\partial l}{\\partial W}$ is a negative number, then we want to _increase_ the weight, since increasing the weight would _decrease_ our loss. In both cases, the equation $ W = W - \\frac{\\partial l}{\\partial W}$ works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropogation - setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we want to make our neural net smarter by updating its weights. We've see that to do that, we need to compute $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial V}$. How do we do this?\n",
    "\n",
    "Well, we know that \n",
    "\n",
    "$$ L = l(p(c(b(a(x, V)), W))) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our good friend the chain rule tells us that: \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial W} = \\frac{\\partial l}{\\partial P} * \\frac{\\partial p}{\\partial C} * \\frac{\\partial c}{\\partial W}  $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial V} = \\frac{\\partial l}{\\partial P} * \\frac{\\partial p}{\\partial C} * \\frac{\\partial c}{\\partial B} * \\frac{\\partial b}{\\partial A} * \\frac{\\partial a}{\\partial V}  $$\n",
    "\n",
    "Each one of these partial derivatives turns out to be simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropogation - step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, let's compute:\n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial P} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since \n",
    "\n",
    "$$ L = l(P) = \\frac{1}{2}(y - P)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then:\n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial P} = -(y - P)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And coding this up is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[-0.3]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "dLdP = -(y - P)\n",
    "array_print(dLdP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_loss_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropagation - step 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Next, let's compute:\n",
    "\n",
    "$$ \\frac{\\partial p}{\\partial C} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall that:\n",
    "\n",
    "$$ P = \\begin{bmatrix} p_1 \\end{bmatrix} = p(c) = \\sigma(c) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A digression on the sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x) * (1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So if\n",
    "\n",
    "$$ p(c) = \\sigma(c) $$\n",
    "\n",
    "then\n",
    "\n",
    "$$ p'(c) = \\sigma(c) * (1 - \\sigma(c)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, coding this up is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.21]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "dPdC = sigmoid(C) * (1-sigmoid(C))\n",
    "array_print(dPdC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_prediction_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Next we want to compute:\n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial W} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall that:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "C &= \\begin{bmatrix} c_1 \\end{bmatrix} \\\\ \n",
    "&= c(W) \\\\\n",
    "&= w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now recall that by $ \\frac{\\partial c}{\\partial W} $ we mean:\n",
    "\n",
    "$$ \\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But since \n",
    "\n",
    "$$ c(W) = w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$ \\frac{\\partial c}{\\partial w_{11}} $, for example, is just $b_1$, $ \\frac{\\partial c}{\\partial w_{21}} $ is just $b_2$, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus,\n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial W} =\n",
    "\\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} = \\begin{bmatrix}b_1 \\\\\n",
    "                  b_2 \\\\\n",
    "                  b_3 \\\\\n",
    "                  b_4\n",
    "                  \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which is just $ B^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, coding this up is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.62]\n",
      " [ 0.64]\n",
      " [ 0.16]\n",
      " [ 0.31]]\n",
      "The dimensions are 4 rows and 1 column\n"
     ]
    }
   ],
   "source": [
    "dCdW = B.T\n",
    "array_print(dCdW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that this has the same dimensions as `W`, which is what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Computing $\\frac{\\partial L}{\\partial W}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now computing $\\frac{\\partial L}{\\partial W}$ is simply a matter of doing the matrix multiplications, which again, by the chain rule, will actually cause the weights to be updated in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[-0.039]\n",
      " [-0.04 ]\n",
      " [-0.01 ]\n",
      " [-0.019]]\n",
      "The dimensions are 4 rows and 1 column\n"
     ]
    }
   ],
   "source": [
    "dLdW = np.dot(dCdW, dLdP * dPdC)\n",
    "array_print(dLdW, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropagation - step 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By the same logic that we applied in Step 3, since:\n",
    "\n",
    "$$ c(W) = w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4 $$\n",
    "\n",
    "we have:\n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial W} =\n",
    "\\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} = \\begin{bmatrix}b_1 \\\\\n",
    "                  b_2 \\\\\n",
    "                  b_3 \\\\\n",
    "                  b_4\n",
    "                  \\end{bmatrix} = B^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And again, since:\n",
    "\n",
    "$$ c(W) = w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4 $$\n",
    "\n",
    "We have:\n",
    "    \n",
    "$$ \\frac{\\partial c}{\\partial B} =\n",
    "\\begin{bmatrix}\\frac{\\partial c}{\\partial b_1} \\\\\n",
    "                  \\frac{\\partial c}{\\partial b_2} \\\\\n",
    "                  \\frac{\\partial c}{\\partial b_3} \\\\\n",
    "                  \\frac{\\partial c}{\\partial b_4}\n",
    "                  \\end{bmatrix} = \\begin{bmatrix}w_{11} \\\\\n",
    "                  w_{21} \\\\\n",
    "                  w_{31} \\\\\n",
    "                  w_{41}\n",
    "                  \\end{bmatrix} = W^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So coding this up simply gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 1.3   0.32 -0.79 -0.15]]\n",
      "The dimensions are 1 row and 4 columns\n"
     ]
    }
   ],
   "source": [
    "dCdB = W.T\n",
    "array_print(dCdB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_c_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Next, we want to compute:\n",
    "\n",
    "$$ \\frac{\\partial b}{\\partial A} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But since:\n",
    "\n",
    "$$ B = b(A) = \\sigma(A) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which is really just shorthand for:\n",
    "\n",
    "$$ B = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\\\ b_4 \\end{bmatrix} = \\begin{bmatrix} \\sigma(a_1) \\\\ \\sigma(a_2) \\\\ \\sigma(a_3) \\\\ \\sigma(a_4) \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since we know that:\n",
    "    \n",
    "$$ \\sigma'(A) = \\sigma(A) * (1 - \\sigma(A)) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then:\n",
    "    \n",
    "$$ \\frac{\\partial b}{\\partial A} = \\begin{bmatrix} \\sigma(a_1) * (1 - \\sigma(a_1) \\\\ \n",
    "\\sigma(a_2) * (1 - \\sigma(a_2) \\\\ \n",
    "\\sigma(a_3) * (1 - \\sigma(a_3) \\\\\n",
    "\\sigma(a_4) * (1 - \\sigma(a_4) \\end{bmatrix} = \\sigma(A) * (1 - \\sigma(A))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.23  0.23  0.13  0.21]]\n",
      "The dimensions are 1 row and 4 columns\n"
     ]
    }
   ],
   "source": [
    "dBdA = sigmoid(A) * (1-sigmoid(A))\n",
    "array_print(dBdA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_b_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, we want to compute the most involved of our partial derivatives:\n",
    "\n",
    "$$ \\frac{\\partial a}{\\partial V} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recalling that:\n",
    "\n",
    "$$ a(X, V) = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But $ a(X, V) $ is itself shorthand for the equations:\n",
    "\n",
    "$$ x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} = a_1 $$\n",
    "$$ x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} = a_2 $$\n",
    "$$ x_1 * v_{13} + x_2 * v_{23} + x_3 * v_{33} = a_3 $$\n",
    "$$ x_1 * v_{14} + x_2 * v_{24} + x_3 * v_{34} = a_4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So $ \\frac{\\partial a}{\\partial V} $ is really shorthand for:\n",
    "\n",
    "$$ \\begin{bmatrix}\\frac{\\partial a}{\\partial v_{11}} & \\frac{\\partial a}{\\partial v_{12}} & \\frac{\\partial a}{\\partial v_{13}} & \\frac{\\partial a}{\\partial v_{14}} \\\\\n",
    "\\frac{\\partial a}{\\partial v_{21}} & \\frac{\\partial a}{\\partial v_{22}} & \\frac{\\partial a}{\\partial v_{23}} & \\frac{\\partial a}{\\partial v_{24}} \\\\\n",
    "\\frac{\\partial a}{\\partial v_{31}} & \\frac{\\partial a}{\\partial v_{32}} & \\frac{\\partial a}{\\partial v_{33}} & \\frac{\\partial a}{\\partial v_{34}} \\\\\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But, note that focusing on just $a_1$ for example:\n",
    "\n",
    "$$ \\frac{\\partial a_1}{\\partial v_{11}} = x_1 $$\n",
    "$$ \\frac{\\partial a_1}{\\partial v_{21}} = x_2 $$\n",
    "$$ \\frac{\\partial a_1}{\\partial v_{31}} = x_3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since again, $ x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} = a_1 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "whereas for $a_2$ and $a_3$\n",
    "\n",
    "$$ \\frac{\\partial a_2}{\\partial v_{11}} = 0 $$\n",
    "$$ \\frac{\\partial a_3}{\\partial v_{11}} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since, for example:\n",
    "\n",
    "$$ x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} = a_2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So if we write: \n",
    "    \n",
    "$$ A = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then $\\frac{\\partial a}{\\partial V}$ ends up being:\n",
    "\n",
    "$$ \\frac{\\partial a}{\\partial V} = \\begin{bmatrix}\n",
    "   \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} &\n",
    "   \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} &\n",
    "   \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} &\n",
    "   \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which in terms of the matrix multiplication that results is the same as writing just:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{\\partial a}{\\partial V} = X^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropagation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which is of course easy to code as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[1]\n",
      " [0]\n",
      " [0]]\n",
      "The dimensions are 3 rows and 1 column\n"
     ]
    }
   ],
   "source": [
    "dAdV = x.T\n",
    "array_print(dAdV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_a_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Computing $\\frac{\\partial l}{\\partial V}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To compute $\\frac{\\partial l}{\\partial V}$, we simply multiply all of these partial derivatives we've calculated together, being careful to use matrix multiplication where necessary and elementwise multiplication where necessary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Computing $\\frac{\\partial l}{\\partial V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[-0.019 -0.005  0.007  0.002]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.   ]]\n",
      "The dimensions are 3 rows and 4 columns\n"
     ]
    }
   ],
   "source": [
    "dLdV = np.dot(dAdV, np.dot(dLdP * dPdC, dCdB) * dBdA)\n",
    "array_print(dLdV, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that this has the same shape as $V$, which is what we want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Updating the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Updating the weights can now be done simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W -= dLdW\n",
    "V -= dLdV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Putting this all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now let's write some functions that train the neural net. The following just wraps around what we've already done, running one batch through the neural net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def learn(V, W, x_batch, y_batch):\n",
    "    # forward pass\n",
    "    A = np.dot(x_batch,V)\n",
    "    B = sigmoid(A)\n",
    "    C = np.dot(B,W)\n",
    "    P = sigmoid(C)\n",
    "    \n",
    "    # loss\n",
    "    L = 0.5 * (y_batch - P) ** 2\n",
    "    \n",
    "    # backpropagation\n",
    "    dLdP = -1.0 * (y_batch - P)\n",
    "    dPdC = sigmoid(C) * (1-sigmoid(C))\n",
    "    dLdC = dLdP * dPdC\n",
    "    dCdW = B.T\n",
    "    dLdW = np.dot(dCdW, dLdC)\n",
    "    dCdB = W.T\n",
    "    dBdA = sigmoid(A) * (1-sigmoid(A))\n",
    "    dAdV = x_batch.T\n",
    "    dLdV = np.dot(dAdV, np.dot(dLdP * dPdC, dCdB) * dBdA)\n",
    "    \n",
    "    # update the weights\n",
    "    W -= dLdW\n",
    "    V -= dLdV\n",
    "    \n",
    "    return V, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Putting this all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, let's play around with some results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch  loss\n",
      "0       0  0.23\n",
      "1      50  0.13\n",
      "2     100  0.05\n",
      "3     150  0.02\n",
      "4     200  0.01\n",
      "5     250  0.01\n",
      "6     300  0.00\n",
      "7     350  0.00\n",
      "8     400  0.00\n",
      "9     450  0.00\n",
      "10    500  0.00\n",
      "The data frame of the predictions this neural net produces is:\n",
      "    Actual  Predicted\n",
      "0     1.0       0.94\n",
      "1     0.0       0.06\n",
      "2     1.0       0.98\n",
      "3     0.0       0.02\n",
      "4     0.0       0.08\n",
      "5     0.0       0.03\n",
      "6     1.0       0.98\n",
      "7     1.0       0.95\n",
      "The accuracy of this trained neural net is 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V, W = train_and_display(X, Y, 500, 4)\n",
    "accuracy_binary(X, Y, V, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1  X2  X3  y\n",
       "0   1   0   0  1\n",
       "1   0   1   0  0\n",
       "2   1   1   0  1\n",
       "3   0   0   0  0\n",
       "4   1   0   1  0\n",
       "5   0   0   1  0\n",
       "6   1   1   1  1\n",
       "7   0   1   1  1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MNIST Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To illustrate that this simple framework really does have all the power of neural nets, let's show that it can solve the MNIST problem, the classic digit recognition task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# \"Fetch\" the data\n",
    "\n",
    "mnist = fetch_mldata('MNIST original') \n",
    "X_mnist, Y_mnist = get_mnist_X_Y(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "train_prop = 0.9\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_mnist, Y_mnist, \n",
    "    test_size=1-train_prop, \n",
    "    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MNIST Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    V, W = train_and_display(X_train, Y_train, 1, 50)\n",
    "    accuracy = accuracy_multiclass(X_test, Y_test, V, W)\n",
    "    print(\"Neural Net MNIST Classification Accuracy:\", round(accuracy, 3) * 100, \"percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MNIST Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Keep in mind we got this accuracy without any \"tricks\": no convolutions, no dropout, no learning rate tuning - in fact, no \"Deep Learning\", since we used only one hidden layer! This shows how far simply having a solid understanding of the mathematics underlying neural nets can get you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transitioning to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "So: we've seen that neural nets can be expressed as mathematical functions. When expressed this way, we can explicity calculate the derivative of each nested function in the neural net to ensure that the weights are updated in the correct way and that the neural net will \"learn\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "But, this involved a lot of steps for a simple neural net with just one hidden layer. If we want to build deeper neural nets, we're going to have to come up with a way of describing neural nets other than as just \"nested functions\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Another way to understand neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that if we were going to code up a deeper net - let's say one with two hidden layers instead of one - we would be repeating some steps:\n",
    "\n",
    "In the middle of the net on the forwards pass, we would be passing the output of a layers through a matrix multiplication, and then through an activation function, twice.\n",
    "\n",
    "Similarly, on the backwards pass, we would twice be passing \"values\" backwards, first based on the derivative of the activation function, and then based on the input to the prior layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neuron illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here's an illustration of what is going on at each neuron in the net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neuron_illustration_backprop.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In addition, this same operation is happening at the same neuron in each \"layer\" of the net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Towards a new way of thinking of neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We might be able to think of neural nets as a series of \"layers\", each of which sends values forward to the layer in front of it and sends values backwards during the backwards pass using the process described on the prior slide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Working backwards from the API we want, we could define a neural net as follows:\n",
    "\n",
    "```\n",
    "layer1 = Layer(args)\n",
    "layer2 = Layer(args)\n",
    "net = Net([layer1, layer2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# New net framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here's how we could define new neural nets:\n",
    "\n",
    "**Neural nets are a series of layers that pass information forwards based on what they receive as <em>input</em> and pass information backwards based on what they receive as the <em>loss</em>.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's code this up. For getting me started on this code, I'm indebted to this guy:\n",
    "\n",
    "<div class=\"visual\">\n",
    "    <img src=\"img/andersbll.png\">\n",
    "</div>\n",
    "\n",
    "[Anders' GitHub](https://github.com/andersbll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Basic neural net framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, a helper function to set up the layers of the neural net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def setup_layers(hidden_neurons, outputs):\n",
    "    layers = []\n",
    "    for i in range(len(hidden_neurons)):\n",
    "        layer = FullyConnected(neurons=hidden_neurons[i], activation_function=sigmoid)\n",
    "        layers.append(layer)\n",
    "\n",
    "    output_layer = FullyConnected(neurons=outputs, activation_function=sigmoid)\n",
    "    layers.append(output_layer)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Basic neural net framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, a simple framework for running observations through a neural net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, hidden_neurons, outputs, loss_function):\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.outputs = outputs\n",
    "        self.loss_function = loss_function\n",
    "        self.layers_setup = False\n",
    "\n",
    "    def forwardpass(self, X, *args):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        # If it is our first time doing a forward pass, set up the\n",
    "        # layers of the network:\n",
    "        if not self.layers_setup:\n",
    "            self.layers = setup_layers(self.hidden_neurons, self.outputs)\n",
    "            self.layers_setup = True\n",
    "\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate the loss on the data and send the result backwards through the net. \"\"\"\n",
    "        loss = self.loss_function(prediction, Y)\n",
    "        return self.loss_function(prediction, Y, bprop=True)\n",
    "\n",
    "    def backpropagate(self, loss):\n",
    "        \"\"\" Backpropagate the loss through the net. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Basic layer definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Next, let's define what a \"layer\" should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "\n",
    "    def fprop(self, input):\n",
    "        \"\"\" Calculate layer output for given input (forward propagation). \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        \"\"\" Calculate input gradient. \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We know that our fully connected layer must have three components:\n",
    "\n",
    "* `__init__` to set it up\n",
    "* `fprop` that will take in a layer input and send it forward to the next layer appropriately\n",
    "* `bprop` that will take in a loss from the following layer and send it backwards through the network appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In addition, during the forward pass and backpropagation, we use the following abbreviations:\n",
    "\n",
    "* LI = \"Layer Input\"\n",
    "* AI = \"Activation Input\"\n",
    "* AO = \"Activation Output\"\n",
    "* LG = \"Layer Gradient\" -> the quantity that a layer is receiving from the layer above it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class FullyConnected(Layer):\n",
    "    \n",
    "    def __init__(self, neurons, activation_function):\n",
    "        self.n_neurons = neurons\n",
    "        self.activation_function = activation_function        \n",
    "        self.iterations = 0\n",
    "        self.weights_initialized = False\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.LI = layer_input\n",
    "        \n",
    "        if not self.weights_initialized:\n",
    "            self.W = np.random.normal(size=(self.LI.shape[1], self.n_neurons))\n",
    "            self.weights_initialized = True\n",
    "        \n",
    "        self.AI = np.dot(self.LI, self.W)\n",
    "        layer_output = self.activation_function(self.AI, bprop=False)\n",
    "        return layer_output\n",
    "    \n",
    "    def bprop(self, layer_gradient):\n",
    "        \n",
    "        dAOdAI = self.activation_function(self.AI, bprop=True)\n",
    "        dLGdAI = layer_gradient * dAOdAI\n",
    "        dAIdW = self.LI.T\n",
    "\n",
    "        weight_update = np.dot(dAIdW, dLGdAI)\n",
    "        W_new = self.W - weight_update\n",
    "        self.W = W_new\n",
    "        \n",
    "        self.iterations += 1\n",
    "        \n",
    "        output_grad = np.dot(dLGdAI, self.W.T)\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll need to redefine our functions to have `bprop` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, bprop=False):\n",
    "    if bprop:\n",
    "        return sigmoid(x) * (1-sigmoid(x))\n",
    "    else:\n",
    "        return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def mean_square_error(prediction, Y, bprop=False):\n",
    "    if bprop:\n",
    "        return -1.0 * (Y - prediction)\n",
    "    else:\n",
    "        return 0.5 * (Y - prediction) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Defining the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist = NeuralNetwork(\n",
    "    hidden_neurons=[50],\n",
    "    outputs=10,\n",
    "    loss_function=mean_square_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from neural_net import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, X_train, Y_train, epochs=5, print_msg=True):\n",
    "    X_train, Y_train = shuffle_data(X_train, Y_train)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        one_epoch(net, X_train, Y_train)\n",
    "        if print_msg:\n",
    "            print(\"Done with epoch\", i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    train(nn_mnist, X_train, Y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    accuracy = net_accuracy(nn_mnist, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes...kind of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deep Learning Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes, in fact, we can use this framework to do Deep Learning. Let's define a neural net with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist_2 = NeuralNetwork(\n",
    "    hidden_neurons=[75, 50, 25],\n",
    "    outputs=10,\n",
    "    loss_function=mean_square_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    train(nn_mnist_2, X_train, Y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    accuracy = net_accuracy(nn_mnist_2, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again, it only \"kind of\" works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deep Learning Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we get to the fun part: tuning our deep learning models using the many tricks researchers have discovered increase the performance of said models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this talk, we'll get through as many of these as we can:\n",
    "\n",
    "* Learning rate tuning\n",
    "* Learning rate decay\n",
    "* Varying learning rates by layer\n",
    "* Learning rate momentum\n",
    "\n",
    "* Dropout\n",
    "* Dropconnect\n",
    "\n",
    "* Weight initializations\n",
    "* Different activation functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Learning rate tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"img/bengio.png\">\n",
    "\n",
    "\"The learning rate is the single most important hyperparameter and one should always make sure it is tuned.\"\n",
    "\n",
    "-[Yoshua Bengio](http://www.iro.umontreal.ca/~bengioy/yoshua_en/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The learning rate is just a number that we multiply the weight update by during each iteration. So if the learning rate is $\\alpha$, the weight update equation for a weight matrix $W$ becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ W = W - \\alpha * \\frac{\\partial l}{\\partial W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Coding this up - learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll modify the `bprop` function within the `FullyConnected` class, we'll add the learning rate to the weight update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class FullyConnectedLR(FullyConnected):\n",
    "    \n",
    "    def bprop(self, layer_gradient):\n",
    "        \n",
    "        dAOdAI = self.activation_function(self.AI, bprop=True)\n",
    "        dLGdAI = layer_gradient * dAOdAI\n",
    "        dAIdW = self.LI.T\n",
    "\n",
    "        weight_update = np.dot(dAIdW, dLGdAI)\n",
    "        \n",
    "        # We now multiply the weight update by a learning rate\n",
    "        W_new = self.W - self.learning_rate * weight_update\n",
    "        self.W = W_new\n",
    "        \n",
    "        self.iterations += 1\n",
    "        \n",
    "        output_grad = np.dot(dLGdAI, self.W.T)\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding this up - learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll modify the a new `setup_layers` function to give each layer a learning rate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def setup_layers(hidden_neurons, outputs, learning_rate=1.0):\n",
    "    layers = []\n",
    "    for i in range(len(hidden_neurons)):\n",
    "        layer = FullyConnectedLR(neurons=hidden_neurons[i], activation_function=sigmoid)\n",
    "        # We give each layer a learning rate during the set up\n",
    "        setattr(layer, \"learning_rate\", learning_rate)\n",
    "        layers.append(layer)\n",
    "\n",
    "    output_layer = FullyConnectedLR(neurons=outputs, activation_function=sigmoid)\n",
    "    setattr(output_layer, \"learning_rate\", learning_rate)\n",
    "    layers.append(output_layer)\n",
    "    return layers   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding this up - learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkLR(NeuralNetwork):\n",
    "    def __init__(self, hidden_neurons, outputs, loss_function, learning_rate):\n",
    "        NeuralNetwork.__init__(self, hidden_neurons, outputs, loss_function)\n",
    "        # Add learning rate as a class variable\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forwardpass(self, X, *args):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        \n",
    "        if not self.layers_setup:\n",
    "            # Add learning rate to the setup_layers function\n",
    "            self.layers = setup_layers(self.hidden_neurons, \n",
    "                                       self.outputs, \n",
    "                                       self.learning_rate)\n",
    "            self.layers_setup = True\n",
    "\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tuning the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_net_lr(learning_rate):\n",
    "    nn_mnist_lr = NeuralNetworkLR(\n",
    "        hidden_neurons=[75, 25],\n",
    "        outputs=10,\n",
    "        loss_function=mean_square_error, \n",
    "        learning_rate=learning_rate)\n",
    "    \n",
    "    train(nn_mnist_lr, X_train, Y_train, epochs=1, print_msg=False)\n",
    "    accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test)\n",
    "    \n",
    "    print(\"The accuracy of a net with learning rate\", \n",
    "          learning_rate, \n",
    "          \"was\", \n",
    "          round(accuracy * 100, 1), \n",
    "          \"percent.\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    learning_rates = np.arange(0.1, 0.6, 0.1)\n",
    "    accuracies = [accuracy_net_lr(lr) for lr in learning_rates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Varying Learning Rates by Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Because backpropagation involves multiplying a value by the derivative of the activation function, gradients (that tell the weights how to update) get smaller and smaller as you go get further from the output layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/sigmoid_deriv_trask.png\">\n",
    "**At most, the gradient can be multiplied by 0.25 at each layer.** More [here](http://iamtrask.github.io/2015/07/12/basic-python-network/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding this up: varying learning rates by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def setup_layers(hidden_neurons, outputs, learning_rate=1.0, learning_rate_layer_decay=1.0):\n",
    "    layers = []\n",
    "    for i in range(len(hidden_neurons)):\n",
    "        layer = FullyConnectedLR(neurons=hidden_neurons[i], activation_function=sigmoid)\n",
    "        # Divide learning rate by layer number\n",
    "        setattr(layer, \"learning_rate\", learning_rate / (learning_rate_layer_decay ** i))\n",
    "        layers.append(layer)\n",
    "\n",
    "    output_layer = FullyConnectedLR(neurons=outputs, activation_function=sigmoid)\n",
    "    # Divide learning rate of last layer by number of layers\n",
    "    setattr(output_layer, \"learning_rate\", learning_rate / (learning_rate_layer_decay ** len(hidden_neurons)))\n",
    "    layers.append(output_layer)\n",
    "    return layers   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding this up: varying learning rates by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkLRDecay(NeuralNetworkLR):\n",
    "    def __init__(self, hidden_neurons, outputs, loss_function, learning_rate, \n",
    "                 learning_rate_layer_decay):\n",
    "        NeuralNetworkLR.__init__(self, hidden_neurons, outputs, loss_function, learning_rate)\n",
    "        self.learning_rate_layer_decay = learning_rate_layer_decay\n",
    "\n",
    "    def forwardpass(self, X, *args):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        \n",
    "        if not self.layers_setup:\n",
    "            # Add learning rate decay to setup_layers function\n",
    "            self.layers = setup_layers(self.hidden_neurons, \n",
    "                                       self.outputs, \n",
    "                                       self.learning_rate,\n",
    "                                       self.learning_rate_layer_decay)\n",
    "            self.layers_setup = True\n",
    "\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkLRDecay(\n",
    "        hidden_neurons=[75, 25],\n",
    "        outputs=10,\n",
    "        loss_function=mean_square_error, \n",
    "        learning_rate=0.3, \n",
    "        learning_rate_layer_decay=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    train(nn_mnist_lr, X_train, Y_train, epochs=1, print_msg=False)\n",
    "    accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning Rate Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The weights in a neural net are updated according to:\n",
    "\n",
    "$$ W = W - \\alpha * \\frac{\\partial l}{\\partial W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that this is equivalent to doing gradient descent with each parameter:\n",
    "\n",
    "<img src=\"img/gradient_descent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is analogous to a ball rolling down a hill. \n",
    "\n",
    "Balls rolling down hills have momentum. So, therefore, should our weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate momentum\n",
    "\n",
    "Let's define our weight update $ \\frac{\\partial L}{\\partial W} $ to be $ U_t $. Then, instead of our weight update being $ U_t $ at each time step, it will be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ U_t + \\mu * U_{t-1} + \\mu^2 * U_{t-2} + ... $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "where $\\mu$ is a decay parameter between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is equivalent to, and often described as, increasing your learning rate when your weight updates are going in the same direction, iteration after iteration, and lowering your learning rate when the opposite is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding this up - learning rate momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def setup_layers(hidden_neurons, outputs, learning_rate=1.0, learning_rate_layer_decay=1.0, momentum=0.1):\n",
    "    layers = []\n",
    "    for i in range(len(hidden_neurons)):\n",
    "        layer = FullyConnectedLRMomentum(neurons=hidden_neurons[i], activation_function=sigmoid)\n",
    "        setattr(layer, \"learning_rate\", learning_rate)\n",
    "        # Add momentum parameter to each layer\n",
    "        setattr(layer, \"momentum\", momentum)\n",
    "        layers.append(layer)\n",
    "\n",
    "    output_layer = FullyConnectedLRMomentum(neurons=outputs, activation_function=sigmoid)\n",
    "    setattr(output_layer, \"learning_rate\", learning_rate)\n",
    "    # Add momentum parameter to last layer\n",
    "    setattr(output_layer, \"momentum\", momentum)\n",
    "    layers.append(output_layer)\n",
    "    return layers   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding this up - learning rate momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class FullyConnectedLRMomentum(FullyConnectedLR):\n",
    "    def __init__(self, neurons, activation_function):\n",
    "        FullyConnectedLR.__init__(self, neurons, activation_function)\n",
    "        self.first = True\n",
    "    \n",
    "    def bprop(self, layer_gradient):\n",
    "        \n",
    "        dAOdAI = self.activation_function(self.AI, bprop=True)\n",
    "        dLGdAI = layer_gradient * dAOdAI\n",
    "        dAIdW = self.LI.T\n",
    "        weight_update_current = np.dot(dAIdW, dLGdAI)\n",
    "        \n",
    "        # Set \"velocity\" on each iteration through the net\n",
    "        if self.first:    \n",
    "            self.velocity = self.learning_rate * weight_update_current\n",
    "            self.first = False\n",
    "        else:\n",
    "            # On most iterations, multiply velocity by momentum, and add learning rate times current update\n",
    "            self.velocity = np.add(self.momentum * self.velocity, \n",
    "                                   self.learning_rate * weight_update_current)\n",
    "        self.W = self.W - self.velocity\n",
    "        \n",
    "        self.iterations += 1\n",
    "        \n",
    "        output_grad = np.dot(dLGdAI, self.W.T)\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding this up - learning rate momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkLRMomentum(NeuralNetworkLRDecay):\n",
    "    def __init__(self, hidden_neurons, outputs, loss_function, learning_rate, \n",
    "                 learning_rate_layer_decay, momentum):\n",
    "        NeuralNetworkLRDecay.__init__(self, hidden_neurons, outputs, loss_function, \n",
    "                                      learning_rate, learning_rate_layer_decay)\n",
    "        # Add momentum as class variable\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def forwardpass(self, X, *args):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        \n",
    "        if not self.layers_setup:\n",
    "            # Add momentum to setup layers function\n",
    "            self.layers = setup_layers(self.hidden_neurons, \n",
    "                                       self.outputs, \n",
    "                                       self.learning_rate,\n",
    "                                       self.learning_rate_layer_decay, \n",
    "                                       self.momentum)\n",
    "    \n",
    "            self.layers_setup = True\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Testing learning rate momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkLRMomentum(\n",
    "        hidden_neurons=[75, 25],\n",
    "        outputs=10,\n",
    "        loss_function=mean_square_error, \n",
    "        learning_rate=0.3, \n",
    "        learning_rate_layer_decay=4,\n",
    "        momentum=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    train(nn_mnist_lr, X_train, Y_train, epochs=1, print_msg=False)\n",
    "    accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Dropout can help prevent neural networks from overfitting. It involves \"dropping\" a portion of the neurons - that is, setting their values to zero - on each forward pass through the network. \n",
    "\n",
    "<img src=\"img/dropout.png\">\n",
    "\n",
    "This nudges the network toward learning \"redundant representations of its data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding this up - dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkDropout(NeuralNetworkLRMomentum):\n",
    "    def __init__(self, hidden_neurons, outputs, loss_function, learning_rate, \n",
    "                 learning_rate_layer_decay, momentum, dropout):\n",
    "        NeuralNetworkLRMomentum.__init__(self, hidden_neurons, outputs, loss_function, \n",
    "                                      learning_rate, learning_rate_layer_decay, momentum)\n",
    "        # Add dropout as a class variable\n",
    "        self.dropout = dropout\n",
    "\n",
    "        \n",
    "    def forwardpass(self, X, predict=False):\n",
    "        # Give \"forwardpass\" a \"predict\" flag so it only applies Dropout during training\n",
    "        \n",
    "        if not self.layers_setup:\n",
    "            self.layers = setup_layers(self.hidden_neurons, \n",
    "                                       self.outputs, \n",
    "                                       self.learning_rate,\n",
    "                                       self.learning_rate_layer_decay, \n",
    "                                       self.momentum)\n",
    "            self.layers_setup = True\n",
    "\n",
    "        X_next = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # Set '1-dropout' proportion of the neurons equal to zero\n",
    "            if self.dropout and not predict:\n",
    "                zero_indices = np.random.choice(range(layer.n_neurons), \n",
    "                                                size=int(layer.n_neurons * (1 - self.dropout)), \n",
    "                                                replace=False)\n",
    "                X_next[:, zero_indices] = 0.0\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Testing dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkDropout(\n",
    "        hidden_neurons=[75, 25],\n",
    "        outputs=10,\n",
    "        loss_function=mean_square_error, \n",
    "        learning_rate=0.3, \n",
    "        learning_rate_layer_decay=4,\n",
    "        momentum=0.2,\n",
    "        dropout=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    train(nn_mnist_lr, X_train, Y_train, epochs=1, print_msg=False)\n",
    "    accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Weight initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each neuron passes some values to the next layer. Those values have some variance. To ensure that the total variance that each layer receives from the prior layer is roughly constant, it has become a best practice to scale the variance of each set of weights by the number of neurons in that layer. So, if there are $N$ neurons in a layer, the weights coming out of that layer will be given variance $\\frac{1}{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding this up - Xavier weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class FullyConnectedXavier(FullyConnectedLRMomentum):\n",
    "    def __init__(self, neurons, activation_function):\n",
    "        FullyConnectedLRMomentum.__init__(self, neurons, activation_function)\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.LI = layer_input\n",
    "        \n",
    "        if not self.weights_initialized:\n",
    "            # Scale the weights' variance by the number of neurons going out of that layer\n",
    "            self.W = np.random.normal(scale=1 / np.sqrt(self.n_neurons), size=(self.LI.shape[1], self.n_neurons))\n",
    "            self.weights_initialized = True\n",
    "        \n",
    "        self.AI = np.dot(self.LI, self.W)\n",
    "        return self.activation_function(self.AI, bprop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding this up - Xavier weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def setup_layers(hidden_neurons, outputs, learning_rate=1.0, learning_rate_layer_decay=1.0, momentum=0.1):\n",
    "    layers = []\n",
    "    for i in range(len(hidden_neurons)):\n",
    "        # Change to use Xavier initialization\n",
    "        layer = FullyConnectedXavier(neurons=hidden_neurons[i], activation_function=sigmoid)\n",
    "        setattr(layer, \"learning_rate\", learning_rate / (learning_rate_layer_decay ** i))\n",
    "        setattr(layer, \"momentum\", momentum)\n",
    "        layers.append(layer)\n",
    "\n",
    "    # Change to use Xavier initialization\n",
    "    output_layer = FullyConnectedXavier(neurons=outputs, activation_function=sigmoid)\n",
    "    setattr(output_layer, \"learning_rate\", learning_rate)\n",
    "    setattr(output_layer, \"momentum\", momentum)\n",
    "    layers.append(output_layer)\n",
    "    return layers   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Testing Xavier weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkDropout(\n",
    "        hidden_neurons=[75, 25],\n",
    "        outputs=10,\n",
    "        loss_function=mean_square_error, \n",
    "        learning_rate=0.3, \n",
    "        learning_rate_layer_decay=4,\n",
    "        momentum=0.2,\n",
    "        dropout=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    train(nn_mnist_lr, X_train, Y_train, epochs=1, print_msg=False)\n",
    "    accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/MNIST_performance.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we can see [here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html), the highest performance model on the MNIST data involved \"Drop Connect\", where a portion of the _weights_ in the neural net are set to zero, as opposed to half the _neurons_.\n",
    "\n",
    "[Not in TensorFlow!](https://stackoverflow.com/questions/37135885/dropconnect-in-tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding this up - DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def apply_drop_connect_weights(weights, drop_connect):\n",
    "    new_weights = weights.copy()\n",
    "    num_weights = new_weights.shape[0] * new_weights.shape[1]\n",
    "    reshaped_weights = np.reshape(new_weights, (num_weights, 1))\n",
    "    zero_indices = np.random.choice(range(num_weights), \n",
    "                                    size=int(num_weights * (1 - drop_connect)), \n",
    "                                    replace=False)\n",
    "    reshaped_weights[zero_indices, :] = 0.0\n",
    "    drop_connected_weights = np.reshape(reshaped_weights, new_weights.shape)\n",
    "    \n",
    "    return drop_connected_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding this up - DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class FullyConnectedDropConnect(FullyConnectedXavier):\n",
    "    def __init__(self, neurons, activation_function, drop_connect):\n",
    "        FullyConnectedXavier.__init__(self, neurons, activation_function)\n",
    "        self.drop_connect = drop_connect\n",
    "    \n",
    "    def fprop(self, layer_input):\n",
    "\n",
    "        self.LI = layer_input\n",
    "        \n",
    "        if not self.weights_initialized:\n",
    "            # Scale the weights' variance by the number of neurons going out of that layer\n",
    "            self.W = np.random.normal(scale=1 / np.sqrt(self.n_neurons), size=(self.LI.shape[1], self.n_neurons))\n",
    "            self.weights_initialized = True\n",
    "        \n",
    "        if self.drop_connect:            \n",
    "            drop_connected_weights = apply_drop_connect_weights(self.W, \n",
    "                                                                self.drop_connect)\n",
    "            self.AI = np.dot(layer_input, \n",
    "                             drop_connected_weights)\n",
    "        else:\n",
    "            self.AI = np.dot(layer_input, self.W)\n",
    "\n",
    "        return self.activation_function(self.AI, bprop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding this up - DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def setup_layers(hidden_neurons, outputs, learning_rate=1.0, learning_rate_layer_decay=1.0, momentum=0.1, \n",
    "                 drop_connect=0.0):\n",
    "    layers = []\n",
    "    for i in range(len(hidden_neurons)):\n",
    "        # Change to use Xavier initialization\n",
    "        layer = FullyConnectedDropConnect(neurons=hidden_neurons[i], \n",
    "                                          activation_function=sigmoid, \n",
    "                                          drop_connect=drop_connect)\n",
    "        setattr(layer, \"learning_rate\", learning_rate / (learning_rate_layer_decay ** i))\n",
    "        setattr(layer, \"momentum\", momentum)\n",
    "        setattr(layer, \"drop_connect\", drop_connect)\n",
    "        layers.append(layer)\n",
    "\n",
    "    # Change to use Xavier initialization\n",
    "    output_layer = FullyConnectedDropConnect(neurons=outputs, \n",
    "                                             activation_function=sigmoid,\n",
    "                                             drop_connect=drop_connect)\n",
    "    setattr(output_layer, \"learning_rate\", learning_rate)\n",
    "    setattr(output_layer, \"momentum\", momentum)\n",
    "    setattr(output_layer, \"drop_connect\", drop_connect)\n",
    "    layers.append(output_layer)\n",
    "    return layers   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding this up - DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkDropConnect(NeuralNetworkDropout):\n",
    "    def __init__(self, hidden_neurons, outputs, loss_function, learning_rate, \n",
    "                 learning_rate_layer_decay, momentum, dropout, drop_connect):\n",
    "        NeuralNetworkDropout.__init__(self, hidden_neurons, outputs, loss_function, \n",
    "                                          learning_rate, learning_rate_layer_decay, momentum,\n",
    "                                          dropout)\n",
    "        # Add drop connect as a class variable\n",
    "        self.drop_connect = drop_connect\n",
    "        \n",
    "    def forwardpass(self, X, predict=False):\n",
    "        \n",
    "        # Add drop_connect to the layers setup\n",
    "        if not self.layers_setup:\n",
    "            self.layers = setup_layers(self.hidden_neurons, \n",
    "                                       self.outputs, \n",
    "                                       self.learning_rate,\n",
    "                                       self.learning_rate_layer_decay, \n",
    "                                       self.momentum, \n",
    "                                       self.drop_connect)\n",
    "            self.layers_setup = True\n",
    "\n",
    "        X_next = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # Set '1-dropout' proportion of the neurons equal to zero\n",
    "            if self.dropout and not predict:\n",
    "                zero_indices = np.random.choice(range(layer.n_neurons), \n",
    "                                                size=int(layer.n_neurons * (1 - self.dropout)), \n",
    "                                                replace=False)\n",
    "                X_next[:, zero_indices] = 0.0\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Testing DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkDropConnect(\n",
    "        hidden_neurons=[75, 25],\n",
    "        outputs=10,\n",
    "        loss_function=mean_square_error, \n",
    "        learning_rate=0.3, \n",
    "        learning_rate_layer_decay=4,\n",
    "        momentum=0.2,\n",
    "        dropout=0.75,\n",
    "        drop_connect=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    train(nn_mnist_lr, X_train, Y_train, epochs=1, print_msg=False)\n",
    "    accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Thanks!**\n",
    "\n",
    "<img src=\"img/professional_headshot.png\" class=\"visual\">\n",
    "\n",
    "[Website](https://www.sethweidman.com) | [Medium](https://medium.com/@sethweidman) | [GitHub](https://github.com/sethHWeidman/) | [Twitter](https://twitter.com/SethHWeidman) | [LinkedIn](https://www.linkedin.com/in/sethhweidman/)\n",
    "\n",
    "seth@sethweidman.com if you have any questions."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "47px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
