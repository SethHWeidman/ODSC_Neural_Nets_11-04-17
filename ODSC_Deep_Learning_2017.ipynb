{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# CSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".expo {\n",
       "  line-height: 150%;\n",
       "}\n",
       "\n",
       ".visual {\n",
       "  width: 600px;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "style = \"\"\"\n",
    "<style>\n",
    ".expo {\n",
    "  line-height: 150%;\n",
    "}\n",
    "\n",
    ".visual {\n",
    "  width: 600px;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\"\"\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_all = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning from Scratch using Python \n",
    "\n",
    "Seth Weidman\n",
    "\n",
    "11/04/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# To begin..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nah..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "This talk will have two parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Part 1: Neural Nets from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We'll implement a basic neural net with one hidden layer, from scratch, and use mathematical principles to get the backpropogation right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We'll show that this same framework can be used to learn MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Part 2: Transitioning to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We'll transition to Deep Learning by changing our mental model of neural nets to be that they contain \"layers\" which pass information backwards and forwards between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We'll show how this framework can be used to construct arbitrarily deep neural networks, and show how these can learn MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: Neural Nets from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "We've all seen diagrams like the following in the context of neural nets:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Part 1: Neural Nets from Scratch\n",
    "\n",
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_v3.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "Many don't fully understand what is going on in this diagram. This talk will attempt to rectify that.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why neural nets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "Let's suppose we need a function that  that can learn a complicated relationship between inputs and outputs:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df, X, Y = generate_x_y()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why Neural Nets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1  X2  X3  y\n",
       "0   1   0   0  1\n",
       "1   0   1   0  0\n",
       "2   1   1   0  1\n",
       "3   0   0   0  0\n",
       "4   1   0   1  0\n",
       "5   0   0   1  0\n",
       "6   1   1   1  1\n",
       "7   0   1   1  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "So: we have eight observations, and a complex relationship between inputs and outputs. Now, let's build a neural net that can \"learn\" this relationship.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why Neural Nets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "How to even begin? Well, let's look at this from as high a level as possible and then progressively dive deeper. First, we want a function $N$ that--based on the data from before--maps inputs to outputs properly, that is:\n",
    "</div>\n",
    "\n",
    "$$ N(1, 0, 0) = 1 $$\n",
    "$$ N(0, 1, 0) = 1 $$\n",
    "$$ N(1, 1, 0) = 1 $$\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "First, let's observe that logistic regression can't do this. That is, there are no parameters $b$, $w_1$, $w_2$, and $w_3$ such that:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$N(x_1, x_2, x_3) = \\frac{1}{1 + e^{b + w_1 * x_1 + w_2 * x_2 + w_3 * x_3}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "This is true for the same reason that logistic regression cannot learn XOR. Our problem is a three dimensional problem since we have three features, but you can easily see in two dimensions that the space is not linearly separable:\n",
    "</div>\n",
    "\n",
    "<div class=\"visual\">\n",
    "  <img src=\"img/xor.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "Of course, we *could* manually do feature engineering...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... but who likes feature engineering???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "Let's have the computer do the feature engineering for us, via a neural net learning hidden features!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's make a prediction using a neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our goal will be to:\n",
    "\n",
    "* Start with our three original features.\n",
    "* Transform them into four \"intermediate features\" using logistic-regression-like transformation.\n",
    "* Take these four \"intermediate features\" and use _them_ to predict our final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do we transform our original three features into an intermediate, or hidden layer? Let's call our intermediate features $a_1$, $a_2$, $a_3$, and $a_4$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want $a_1$, for example, to be a linear combination of $x_1$, $x_2$, and $x_3$ - that is, we want some weights $v_{11}$, $v_{12}$, and $v_{13}$ so that:\n",
    "\n",
    "$$ a_1 = x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} $$\n",
    "\n",
    "and similarly for $a_2$, $a_3$, and $a_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A way to concisely express this is to define your features as a vector\n",
    "\n",
    "$$ X = \\begin{bmatrix}x_1 & x_2 & x_3\\end{bmatrix} $$\n",
    "\n",
    "This should be intuitive, since $X$ is already a row in your data! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then, multiplying this vector by a matrix $V$:\n",
    "\n",
    "$$ V = \\begin{bmatrix}v_{11} & v_{12} & v_{13} & v_{14} \\\\\n",
    "                      v_{21} & v_{22} & v_{23} & v_{24} \\\\\n",
    "                      v_{31} & v_{32} & v_{33} & v_{34}\n",
    "                      \\end{bmatrix} $$\n",
    "\n",
    "gives us what we want, since \"$A = X * V$\" is equivalent to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ a_1 = x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} $$\n",
    "$$ a_2 = x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} $$\n",
    "$$ a_3 = x_1 * v_{13} + x_2 * v_{23} + x_3 * v_{33} $$\n",
    "$$ a_4 = x_1 * v_{14} + x_2 * v_{24} + x_3 * v_{34} $$\n",
    "\n",
    "which is what we want in order to get four intermediate features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's code this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[1 0 0]]\n",
      "The dimensions are 1 row and 3 columns\n"
     ]
    }
   ],
   "source": [
    "x = np.array(X[0], ndmin=2)\n",
    "array_print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.54 -0.61 -1.48 -1.  ]\n",
      " [ 0.02  0.1   0.57 -0.7 ]\n",
      " [ 0.57 -0.69 -0.01  0.11]]\n",
      "The dimensions are 3 rows and 4 columns\n"
     ]
    }
   ],
   "source": [
    "V = np.random.randn(3, 4)\n",
    "array_print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.54 -0.61 -1.48 -1.  ]]\n",
      "The dimensions are 1 row and 4 columns\n"
     ]
    }
   ],
   "source": [
    "A = np.dot(x, V)\n",
    "array_print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we?\n",
    "\n",
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_first_layer.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 2: feeding these intermediate features through an \"activation function\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We're going to use a classic, easy-to-understand activation function, though one that is not often used in cutting-edge applications: the sigmoid function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 2: feeding these intermediate features through an \"activation function\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ B = \\sigma(A) $$ or\n",
    "\n",
    "$$ b_1 = \\sigma(a_1) $$\n",
    "$$ b_2 = \\sigma(a_2) $$\n",
    "$$ b_3 = \\sigma(a_3) $$\n",
    "$$ b_4 = \\sigma(a_4) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 2: feeding these intermediate features through an \"activation function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.63  0.35  0.18  0.27]]\n",
      "The dimensions are 1 row and 4 columns\n"
     ]
    }
   ],
   "source": [
    "B = sigmoid(A)\n",
    "array_print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we?\n",
    "\n",
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_first_sigmoid.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3: use these intermediate features as a linear combination to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll multiply these \"sigmoided\" results by another matrix $W$ to get a single output. Since we want to transform 4 features down into 1, we can use a 4 x 1 matrix:\n",
    "\n",
    "$$ W = \\begin{bmatrix}w_{11} \\\\\n",
    "                      w_{21} \\\\\n",
    "                      w_{31} \\\\\n",
    "                      w_{41}\n",
    "                      \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3: use these intermediate features as a linear combination to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And since we want the result to be:\n",
    "\n",
    "$$ c_1 = w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3: use these intermediate features as a linear combination to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is equivalent to writing:\n",
    "\n",
    "$$ C = B * W $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "or:\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "c_1 \\end{bmatrix} = \n",
    "\\begin{bmatrix}b_1 &\n",
    "                  b_2 &\n",
    "                  b_3 &\n",
    "                  b_4\n",
    "                  \\end{bmatrix} * \n",
    "\\begin{bmatrix}w_{11} \\\\\n",
    "               w_{21} \\\\\n",
    "               w_{31} \\\\\n",
    "               w_{41}\n",
    "               \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3: use these intermediate features as a linear combination to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So we can simply code this up as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.82]\n",
      " [ 0.37]\n",
      " [-0.59]\n",
      " [-0.77]]\n",
      "The dimensions are 4 rows and 1 column\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(4, 1)\n",
    "array_print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.33]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "C = np.dot(B, W)\n",
    "array_print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_second_layer.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 4: sigmoid this to make a final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically, we want:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ p_1 = \\sigma(c_1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So we can simply code this up as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.58]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "P = sigmoid(C)\n",
    "array_print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_final_prediction.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 5: compute the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically, we'll compute mean squared error loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ L = \\frac{1}{2}(y - P)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And coding this up is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.09]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "y = np.array(Y[0], ndmin=2)\n",
    "L = 0.5 * (y - P) ** 2\n",
    "array_print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_loss.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have made our prediction and computed our loss, $L$. Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall: each \"step\" is just a function applied to some input that results in some output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we write out what we just did in terms of mathematical functions, we could write it as:\n",
    "\n",
    "\\begin{align}\n",
    "A &= a(x, V) \\\\\n",
    "B &= b(A) \\\\\n",
    "C &= c(B, W) \\\\\n",
    "P &= p(C) \\\\\n",
    "L &= l(P)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, say we have a neural net with just one hidden layer. We could write the loss of a neural net on a given observation $ x $ as:\n",
    "\n",
    "$$ L = l(p(c(b(a(x, V)), W))) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically, we _want_ to change the weights in such a way that the loss will be reduced during the next iteration. The equations:\n",
    "\n",
    "$$ W = W - \\frac{\\partial l}{\\partial W}$$\n",
    "\n",
    "$$ V = V - \\frac{\\partial l}{\\partial V}$$\n",
    "\n",
    "do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice that this \"makes sense\":\n",
    "\n",
    "* If $\\frac{\\partial l}{\\partial W}$ is a positive number, then we want to _decrease_ the weight, since increasing the weight would _increase_ our loss. That is exactly what the equation $ W = W - \\frac{\\partial l}{\\partial W}$ does.\n",
    "* Similarly, if $\\frac{\\partial l}{\\partial W}$ is a negative number, then we want to _increase_ the weight, since increasing the weight would _decrease_ our loss. In both cases, the equation $ W = W - \\frac{\\partial l}{\\partial W}$ works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropogation - setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we want to make our neural net smarter by updating its weights. We've see that to do that, we need to compute $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial V}$. How do we do this?\n",
    "\n",
    "Well, we know that \n",
    "\n",
    "$$ L = l(p(c(b(a(x, V)), W))) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our good friend the chain rule tells us that: \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial W} = \\frac{\\partial l}{\\partial P} * \\frac{\\partial p}{\\partial C} * \\frac{\\partial c}{\\partial W}  $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial V} = \\frac{\\partial l}{\\partial P} * \\frac{\\partial p}{\\partial C} * \\frac{\\partial c}{\\partial B} * \\frac{\\partial b}{\\partial A} * \\frac{\\partial a}{\\partial V}  $$\n",
    "\n",
    "Each one of these partial derivatives turns out to be simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropogation - step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, let's compute:\n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial P} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since \n",
    "\n",
    "$$ L = l(P) = \\frac{1}{2}(y - P)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then:\n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial P} = -(y - P)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And coding this up is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[-0.42]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "dLdP = -(y - P)\n",
    "array_print(dLdP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_loss_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropogation - step 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Next, let's compute:\n",
    "\n",
    "$$ \\frac{\\partial p}{\\partial C} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall that:\n",
    "\n",
    "$$ P = \\begin{bmatrix} p_1 \\end{bmatrix} = p(c) = \\sigma(c) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A digression on the sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x) * (1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So if\n",
    "\n",
    "$$ p(c) = \\sigma(c) $$\n",
    "\n",
    "then\n",
    "\n",
    "$$ p'(c) = \\sigma(c) * (1 - \\sigma(c)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, coding this up is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.24]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "dPdC = sigmoid(C) * (1-sigmoid(C))\n",
    "array_print(dPdC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_prediction_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Next we want to compute:\n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial W} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall that:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "C &= \\begin{bmatrix} c_1 \\end{bmatrix} \\\\ \n",
    "&= c(W) \\\\\n",
    "&= w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now recall that by $ \\frac{\\partial c}{\\partial W} $ we mean:\n",
    "\n",
    "$$ \\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But since \n",
    "\n",
    "$$ c(W) = w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$ \\frac{\\partial c}{\\partial w_{11}} $, for example, is just $b_1$, $ \\frac{\\partial c}{\\partial w_{21}} $ is just $b_2$, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus,\n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial W} =\n",
    "\\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} = \\begin{bmatrix}b_1 \\\\\n",
    "                  b_2 \\\\\n",
    "                  b_3 \\\\\n",
    "                  b_4\n",
    "                  \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which is just $ B^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, coding this up is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.63]\n",
      " [ 0.35]\n",
      " [ 0.18]\n",
      " [ 0.27]]\n",
      "The dimensions are 4 rows and 1 column\n"
     ]
    }
   ],
   "source": [
    "dCdW = B.T\n",
    "array_print(dCdW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that this has the same dimensions as `W`, which is what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Computing $\\frac{\\partial L}{\\partial W}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now computing $\\frac{\\partial L}{\\partial W}$ is simply a matter of doing the matrix multiplications, which again, by the chain rule, will actually cause the weights to be updated in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[-0.064]\n",
      " [-0.036]\n",
      " [-0.019]\n",
      " [-0.027]]\n",
      "The dimensions are 4 rows and 1 column\n"
     ]
    }
   ],
   "source": [
    "dLdW = np.dot(dCdW, dLdP * dPdC)\n",
    "array_print(dLdW, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropogation - step 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By the same logic that we applied in Step 3, since:\n",
    "\n",
    "$$ c(W) = w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4 $$\n",
    "\n",
    "we have:\n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial W} =\n",
    "\\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} = \\begin{bmatrix}b_1 \\\\\n",
    "                  b_2 \\\\\n",
    "                  b_3 \\\\\n",
    "                  b_4\n",
    "                  \\end{bmatrix} = B^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And again, since:\n",
    "\n",
    "$$ c(W) = w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4 $$\n",
    "\n",
    "We have:\n",
    "    \n",
    "$$ \\frac{\\partial c}{\\partial B} =\n",
    "\\begin{bmatrix}\\frac{\\partial c}{\\partial b_1} \\\\\n",
    "                  \\frac{\\partial c}{\\partial b_2} \\\\\n",
    "                  \\frac{\\partial c}{\\partial b_3} \\\\\n",
    "                  \\frac{\\partial c}{\\partial b_4}\n",
    "                  \\end{bmatrix} = \\begin{bmatrix}w_{11} \\\\\n",
    "                  w_{21} \\\\\n",
    "                  w_{31} \\\\\n",
    "                  w_{41}\n",
    "                  \\end{bmatrix} = W^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So coding this up simply gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.82  0.37 -0.59 -0.77]]\n",
      "The dimensions are 1 row and 4 columns\n"
     ]
    }
   ],
   "source": [
    "dCdB = W.T\n",
    "array_print(dCdB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_c_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Next, we want to compute:\n",
    "\n",
    "$$ \\frac{\\partial b}{\\partial A} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But since:\n",
    "\n",
    "$$ B = b(A) = \\sigma(A) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which is really just shorthand for:\n",
    "\n",
    "$$ B = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\\\ b_4 \\end{bmatrix} = \\begin{bmatrix} \\sigma(a_1) \\\\ \\sigma(a_2) \\\\ \\sigma(a_3) \\\\ \\sigma(a_4) \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since we know that:\n",
    "    \n",
    "$$ \\sigma'(A) = \\sigma(A) * (1 - \\sigma(A)) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then:\n",
    "    \n",
    "$$ \\frac{\\partial b}{\\partial A} = \\begin{bmatrix} \\sigma(a_1) * (1 - \\sigma(a_1) \\\\ \n",
    "\\sigma(a_2) * (1 - \\sigma(a_2) \\\\ \n",
    "\\sigma(a_3) * (1 - \\sigma(a_3) \\\\\n",
    "\\sigma(a_4) * (1 - \\sigma(a_4) \\end{bmatrix} = \\sigma(A) * (1 - \\sigma(A))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.23  0.23  0.15  0.2 ]]\n",
      "The dimensions are 1 row and 4 columns\n"
     ]
    }
   ],
   "source": [
    "dBdA = sigmoid(A) * (1-sigmoid(A))\n",
    "array_print(dBdA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_b_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, we want to compute the most involved of our partial derivatives:\n",
    "\n",
    "$$ \\frac{\\partial a}{\\partial V} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recalling that:\n",
    "\n",
    "$$ a(X, V) = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But $ a(X, V) $ is itself shorthand for the equations:\n",
    "\n",
    "$$ x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} = a_1 $$\n",
    "$$ x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} = a_2 $$\n",
    "$$ x_1 * v_{13} + x_2 * v_{23} + x_3 * v_{33} = a_3 $$\n",
    "$$ x_1 * v_{14} + x_2 * v_{24} + x_3 * v_{34} = a_4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So $ \\frac{\\partial a}{\\partial V} $ is really shorthand for:\n",
    "\n",
    "$$ \\begin{bmatrix}\\frac{\\partial a}{\\partial v_{11}} & \\frac{\\partial a}{\\partial v_{12}} & \\frac{\\partial a}{\\partial v_{13}} & \\frac{\\partial a}{\\partial v_{14}} \\\\\n",
    "\\frac{\\partial a}{\\partial v_{21}} & \\frac{\\partial a}{\\partial v_{22}} & \\frac{\\partial a}{\\partial v_{23}} & \\frac{\\partial a}{\\partial v_{24}} \\\\\n",
    "\\frac{\\partial a}{\\partial v_{31}} & \\frac{\\partial a}{\\partial v_{32}} & \\frac{\\partial a}{\\partial v_{33}} & \\frac{\\partial a}{\\partial v_{34}} \\\\\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But, note that focusing on just $a_1$ for example:\n",
    "\n",
    "$$ \\frac{\\partial a_1}{\\partial v_{11}} = x_1 $$\n",
    "$$ \\frac{\\partial a_1}{\\partial v_{21}} = x_2 $$\n",
    "$$ \\frac{\\partial a_1}{\\partial v_{31}} = x_3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since again, $ x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} = a_1 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "whereas for $a_2$ and $a_3$\n",
    "\n",
    "$$ \\frac{\\partial a_2}{\\partial v_{11}} = 0 $$\n",
    "$$ \\frac{\\partial a_3}{\\partial v_{11}} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since, for example:\n",
    "\n",
    "$$ x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} = a_2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So if we write: \n",
    "    \n",
    "$$ A = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then $\\frac{\\partial a}{\\partial V}$ ends up being:\n",
    "\n",
    "$$ \\frac{\\partial a}{\\partial V} = \\begin{bmatrix}\n",
    "   \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} &\n",
    "   \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} &\n",
    "   \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} &\n",
    "   \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which in terms of the matrix multiplication that results is the same as writing just:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{\\partial a}{\\partial V} = X^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which is of course easy to code as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[1]\n",
      " [0]\n",
      " [0]]\n",
      "The dimensions are 3 rows and 1 column\n"
     ]
    }
   ],
   "source": [
    "dAdV = x.T\n",
    "array_print(dAdV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_a_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Computing $\\frac{\\partial l}{\\partial V}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To compute $\\frac{\\partial l}{\\partial V}$, we simply multiply all of these partial derivatives we've calculated together, being careful to use matrix multiplication where necessary and elementwise multiplication where necessary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Computing $\\frac{\\partial l}{\\partial V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[-0.02 -0.01  0.01  0.02]\n",
      " [ 0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]]\n",
      "The dimensions are 3 rows and 4 columns\n"
     ]
    }
   ],
   "source": [
    "dLdV = np.dot(dAdV, np.dot(dLdP * dPdC, dCdB) * dBdA)\n",
    "array_print(dLdV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that this has the same shape as $V$, which is what we want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Updating the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Updating the weights can now be done simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W -= dLdW\n",
    "V -= dLdV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Putting this all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now let's write some functions that train the neural net. The following just wraps around what we've already done, running one batch through the neural net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def learn(V, W, x_batch, y_batch):\n",
    "    # forward pass\n",
    "    A = np.dot(x_batch,V)\n",
    "    B = sigmoid(A)\n",
    "    C = np.dot(B,W)\n",
    "    P = sigmoid(C)\n",
    "    \n",
    "    # loss\n",
    "    L = 0.5 * (y_batch - P) ** 2\n",
    "    \n",
    "    # backpropogation\n",
    "    dLdP = -1.0 * (y_batch - P)\n",
    "    dPdC = sigmoid(C) * (1-sigmoid(C))\n",
    "    dLdC = dLdP * dPdC\n",
    "    dCdW = B.T\n",
    "    dLdW = np.dot(dCdW, dLdC)\n",
    "    dCdB = W.T\n",
    "    dBdA = sigmoid(A) * (1-sigmoid(A))\n",
    "    dAdV = x_batch.T\n",
    "    dLdV = np.dot(dAdV, np.dot(dLdP * dPdC, dCdB) * dBdA)\n",
    "    \n",
    "    # update the weights\n",
    "    W -= dLdW\n",
    "    V -= dLdV\n",
    "    \n",
    "    return V, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Putting this all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, let's play around with some results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch  loss\n",
      "0       0  0.23\n",
      "1      50  0.13\n",
      "2     100  0.05\n",
      "3     150  0.02\n",
      "4     200  0.01\n",
      "5     250  0.01\n",
      "6     300  0.00\n",
      "7     350  0.00\n",
      "8     400  0.00\n",
      "9     450  0.00\n",
      "10    500  0.00\n",
      "The data frame of the predictions this neural net produces is:\n",
      "    Actual  Predicted\n",
      "0     1.0       0.94\n",
      "1     0.0       0.06\n",
      "2     1.0       0.98\n",
      "3     0.0       0.02\n",
      "4     0.0       0.08\n",
      "5     0.0       0.03\n",
      "6     1.0       0.98\n",
      "7     1.0       0.95\n",
      "The accuracy of this trained neural net is 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V, W = train_and_display(X, Y, 500, 4)\n",
    "accuracy_binary(X, Y, V, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MNIST Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To illustrate that this simple framework really does have all the power of neural nets, let's show that it can solve the MNIST problem, the classic digit recognition task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# \"Fetch\" the data\n",
    "\n",
    "mnist = fetch_mldata('MNIST original') \n",
    "X_mnist, Y_mnist = get_mnist_X_Y(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "train_prop = 0.9\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_mnist, Y_mnist, \n",
    "    test_size=1-train_prop, \n",
    "    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MNIST Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    V, W = train_and_display(X_train, Y_train, 10, 50)\n",
    "    accuracy = accuracy_multiclass(X_test, Y_test, V, W)\n",
    "    print(\"Neural Net MNIST Classification Accuracy:\", round(accuracy, 3) * 100, \"percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MNIST Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Keep in mind we got this accuracy without any \"tricks\": no convolutions, no dropout, no learning rate tuning - in fact, no \"Deep Learning\", since we used only one hidden layer! This shows how far simply having a solid understanding of the mathematics underlying neural nets can get you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transitioning to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "So: we've seen that neural nets can be expressed as mathematical functions. When expressed this way, we can explicity calculate the derivative of each nested function in the neural net to ensure that the weights are updated in the correct way and that the neural net will \"learn\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "But, this involved a lot of steps for a simple neural net with just one hidden layer. If we want to build deeper neural nets, we're going to have to come up with a way of describing neural nets other than as just \"nested functions\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Another way to understand neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that if we were going to code up a deeper net - let's say one with two hidden layers instead of one - we would be repeating some steps:\n",
    "\n",
    "In the middle of the net on the forwards pass, we would be passing the output of a layers through a matrix multiplication, and then through an activation function, twice.\n",
    "\n",
    "Similarly, on the backwards pass, we would twice be passing \"values\" backwards, first based on the derivative of the activation function, and then based on the input to the prior layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neuron illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here's an illustration of what is going on at each neuron in the net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neuron_illustration_backprop.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In addition, this same operation is happening at the same neuron in each \"layer\" of the net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Towards a new way of thinking of neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We might be able to think of neural nets as a series of \"layers\", each of which sends values forward to the layer in front of it and sends values backwards during the backwards pass using the process described on the prior slide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Working backwards from the API we want, we could define a neural net as follows:\n",
    "\n",
    "```\n",
    "layer1 = Layer(args)\n",
    "layer2 = Layer(args)\n",
    "net = Net([layer1, layer2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# New net framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here's how we could define new neural nets:\n",
    "\n",
    "**Neural nets are a series of layers that pass information forwards based on what they receive as <em>input</em> and pass information backwards based on what they receive as the <em>loss</em>.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's code this up. For getting me started on this code, I'm indebted to this guy:\n",
    "\n",
    "<div class=\"visual\">\n",
    "    <img src=\"img/andersbll.png\">\n",
    "</div>\n",
    "\n",
    "[Anders' GitHub](https://github.com/andersbll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Basic neural net framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, a helper function to set up the layers of the neural net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def setup_layers(hidden_neurons, outputs):\n",
    "    layers = []\n",
    "    for i in range(len(hidden_neurons)):\n",
    "        layer = FullyConnected(neurons=hidden_neurons[i], activation_function=sigmoid)\n",
    "        layers.append(layer)\n",
    "\n",
    "    output_layer = FullyConnected(neurons=outputs, activation_function=sigmoid)\n",
    "    layers.append(output_layer)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Basic neural net framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, a simple framework for running observations through a neural net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, hidden_neurons, outputs, loss_function):\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.outputs = outputs\n",
    "        self.loss_function = loss_function\n",
    "        self.layers_setup = False\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        # If it is our first time doing a forward pass, set up the\n",
    "        # layers of the network:\n",
    "        if not self.layers_setup:\n",
    "            self.layers = setup_layers(self.hidden_neurons, self.outputs)\n",
    "            self.layers_setup = True\n",
    "\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate the loss on the data and send the result backwards through the net. \"\"\"\n",
    "        loss = self.loss_function(prediction, Y)\n",
    "        return self.loss_function(prediction, Y, bprop=True)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Backpropogate the loss through the net. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Basic layer definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Next, let's define what a \"layer\" should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "\n",
    "    def fprop(self, input):\n",
    "        \"\"\" Calculate layer output for given input (forward propagation). \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        \"\"\" Calculate input gradient. \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We know that our fully connected layer must have three components:\n",
    "\n",
    "* `__init__` to set it up\n",
    "* `fprop` that will take in a layer input and send it forward to the next layer appropriately\n",
    "* `bprop` that will take in a loss from the following layer and send it backwards through the network appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In addition, during the forward pass and backpropogation, we use the following abbreviations:\n",
    "\n",
    "* LI = \"Layer Input\"\n",
    "* AI = \"Activation Input\"\n",
    "* AO = \"Activation Output\"\n",
    "* LG = \"Layer Gradient\" -> the quantity that a layer is receiving from the layer above it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class FullyConnected(Layer):\n",
    "    \n",
    "    def __init__(self, neurons, activation_function):\n",
    "        self.n_neurons = neurons\n",
    "        self.activation_function = activation_function        \n",
    "        self.iterations = 0\n",
    "        self.weights_initialized = False\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.LI = layer_input\n",
    "        \n",
    "        if not self.weights_initialized:\n",
    "            self.W = np.random.normal(size=(self.LI.shape[1], self.n_neurons))\n",
    "            self.weights_initialized = True\n",
    "        \n",
    "        self.AI = np.dot(self.LI, self.W)\n",
    "        return self.activation_function(self.AI, bprop=False)\n",
    "    \n",
    "    def bprop(self, layer_gradient):\n",
    "        \n",
    "        dAOdAI = self.activation_function(self.AI, bprop=True)\n",
    "        dLGdAI = layer_gradient * dAOdAI\n",
    "        dAIdW = self.LI.T\n",
    "\n",
    "        weight_update = np.dot(dAIdW, dLGdAI)\n",
    "        W_new = self.W - weight_update\n",
    "        self.W = W_new\n",
    "        \n",
    "        self.iterations += 1\n",
    "        \n",
    "        output_grad = np.dot(dLGdAI, self.W.T)\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll need to redefine our functions to have `bprop` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, bprop=False):\n",
    "    if bprop:\n",
    "        return sigmoid(x) * (1-sigmoid(x))\n",
    "    else:\n",
    "        return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def mean_square_error(prediction, Y, bprop=False):\n",
    "    if bprop:\n",
    "        return -1.0 * (Y - prediction)\n",
    "    else:\n",
    "        return 0.5 * (Y - prediction) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Defining the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist = NeuralNetwork(\n",
    "    hidden_neurons=[50],\n",
    "    outputs=10,\n",
    "    loss_function=mean_square_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_mnist = NeuralNetwork(\n",
    "    hidden_neurons=[50],\n",
    "    outputs=10,\n",
    "    loss_function=cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from neural_net import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, X_train, Y_train, epochs=5, print_msg=True):\n",
    "    X_train, Y_train = shuffle_data(X_train, Y_train)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        one_epoch(net, X_train, Y_train)\n",
    "        if print_msg:\n",
    "            print(\"Done with epoch\", i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO #7: Investigate 0 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all=True\n",
    "def cross_entropy(prediction, Y, bprop=False):\n",
    "    if bprop:\n",
    "        return -1.0 * Y / (prediction + 0.01) + (1.0 - Y) / (1.0 - (0.01 + prediction))\n",
    "    else:\n",
    "        return -1.0 * Y * np.log(prediction+0.01) - (1.0 - Y) * (np.log(1.01 - prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with epoch 1\n",
      "Done with epoch 2\n",
      "Done with epoch 3\n",
      "Done with epoch 4\n",
      "Done with epoch 5\n"
     ]
    }
   ],
   "source": [
    "if train_all:\n",
    "    train(nn_mnist, X_train, Y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forwardpass() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-bd6b0ed8bcab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain_all\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_mnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/development/ODSC_Neural_Nets_11-04-17/neural_net.py\u001b[0m in \u001b[0;36mnet_accuracy\u001b[0;34m(net, X_test, Y_test, predict)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnet_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mactuals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forwardpass() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "if train_all:\n",
    "    accuracy = net_accuracy(nn_mnist, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net MNIST Classification Accuracy: 11.1 percent\n"
     ]
    }
   ],
   "source": [
    "P = nn_mnist.forwardpass(X_test)\n",
    "preds = [np.argmax(x) for x in P]\n",
    "actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "print(\"Neural Net MNIST Classification Accuracy:\", round(accuracy, 3) * 100, \"percent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes...kind of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deep Learning Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes, in fact, we can use this framework to do Deep Learning. Let's define a neural net with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist_2 = NeuralNetwork(\n",
    "    hidden_neurons=[75, 25],\n",
    "    outputs=10,\n",
    "    loss_function=mean_square_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    train(nn_mnist_2, X_train, Y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    accuracy = net_accuracy(nn_mnist_2, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again, it only \"kind of\" works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deep Learning Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we get to the fun part: tuning our deep learning models using the many tricks researchers have discovered increase the performance of said models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this talk, we'll get through as many of these as we can:\n",
    "\n",
    "* Learning rate tuning\n",
    "* Learning rate decay\n",
    "* Varying learning rates by layer\n",
    "* Learning rate momentum\n",
    "\n",
    "* Dropout\n",
    "* Dropconnect\n",
    "\n",
    "* Weight initializations\n",
    "* Different activation functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Learning rate tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/bengio.png\">\n",
    "\n",
    "\"The learning rate is the single most important hyperparameter and one should always make sure it is tuned.\"\n",
    "\n",
    "-[Yoshua Bengio](http://www.iro.umontreal.ca/~bengioy/yoshua_en/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The learning rate is just a number that we multiply the weight update by during each iteration. So if the learning rate is $\\alpha$, the weight update equation for a weight matrix $W$ becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ W = W - \\alpha * \\frac{\\partial l}{\\partial W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Coding this up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll modify the `bprop` function within the `FullyConnected` class, we'll add the learning rate to the weight update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class FullyConnectedLR(FullyConnected):\n",
    "    \n",
    "    def bprop(self, layer_gradient):\n",
    "        \n",
    "        dAOdAI = self.activation_function(self.AI, bprop=True)\n",
    "        dLGdAI = layer_gradient * dAOdAI\n",
    "        dAIdW = self.LI.T\n",
    "\n",
    "        weight_update = np.dot(dAIdW, dLGdAI)\n",
    "        W_new = self.W - self.learning_rate * weight_update\n",
    "        self.W = W_new\n",
    "        \n",
    "        self.iterations += 1\n",
    "        \n",
    "        output_grad = np.dot(dLGdAI, self.W.T)\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding this up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll modify the a new `setup_layers` function to give each layer a learning rate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def setup_layers(hidden_neurons, outputs, learning_rate=1.0):\n",
    "    layers = []\n",
    "    for i in range(len(hidden_neurons)):\n",
    "        layer = FullyConnectedLR(neurons=hidden_neurons[i], activation_function=sigmoid)\n",
    "        setattr(layer, \"learning_rate\", learning_rate)\n",
    "        layers.append(layer)\n",
    "\n",
    "    output_layer = FullyConnectedLR(neurons=outputs, activation_function=sigmoid)\n",
    "    setattr(output_layer, \"learning_rate\", learning_rate)\n",
    "    layers.append(output_layer)\n",
    "    return layers   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding this up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the `NeuralNetwork` class, we change `__init__` to add a learning rate and `forwardpass` to add the new `setup_layers` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkLR(NeuralNetwork):\n",
    "    def __init__(self, hidden_neurons, outputs, loss_function, learning_rate):\n",
    "        NeuralNetwork.__init__(self, hidden_neurons, outputs, loss_function)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        \n",
    "        if not self.layers_setup:\n",
    "            self.layers = setup_layers(self.hidden_neurons, \n",
    "                                       self.outputs, \n",
    "                                       self.learning_rate)\n",
    "            self.layers_setup = True\n",
    "\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_net_lr(learning_rate):\n",
    "    nn_mnist_lr = NeuralNetworkLR(\n",
    "        hidden_neurons=[75, 25],\n",
    "        outputs=10,\n",
    "        loss_function=mean_square_error, \n",
    "        learning_rate=learning_rate)\n",
    "    \n",
    "    train(nn_mnist_lr, X_train, Y_train, epochs=2, print=False)\n",
    "    accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test)\n",
    "    \n",
    "    print(\"The accuracy of a net with learning rate\", \n",
    "          learning_rate, \n",
    "          \"was\", \n",
    "          round(accuracy * 100, 1), \n",
    "          \"percent.\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    learning_rates = np.arange(0.1, 1.6, 0.1)\n",
    "    accuracies = [accuracy_net_lr(lr) for lr in learning_rates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO #1: Clean up this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Learning rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We may want to decay the learning rate over time, so that the net learns faster at first and then slows its learning over time. See [here](http://cs231n.github.io/neural-networks-3/#anneal) for example. \n",
    "\n",
    "Here's how we might implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def train_learning_rate_decay(net, X_train, Y_train, epochs=5, print_msg=True,\n",
    "                              learning_rate_anneal=2.0):\n",
    "    X_train, Y_train = shuffle_data(X_train, Y_train)\n",
    "    \n",
    "    learning_rate_schedule = np.linspace(net.learning_rate * learning_rate_anneal, \n",
    "                                         net.learning_rate / learning_rate_anneal, \n",
    "                                         epochs)\n",
    "    for i in range(epochs):\n",
    "        setattr(net, \"learning_rate\", learning_rate_schedule[i])\n",
    "        if print_msg:\n",
    "            print(\"Learning rate on epoch\", i+1, \n",
    "                  \"is\", net.learning_rate)\n",
    "        one_epoch(net, X_train, Y_train)\n",
    "        if print_msg:\n",
    "            print(\"Done with epoch\", i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Testing learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkLR(\n",
    "        hidden_neurons=[75, 25],\n",
    "        outputs=10,\n",
    "        loss_function=mean_square_error, \n",
    "        learning_rate=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    train_learning_rate_decay(nn_mnist_lr, X_train, Y_train, epochs=5, print_msg=True,\n",
    "                              learning_rate_anneal=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Testing learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test)\n",
    "    print(\"Neural Net MNIST Classification Accuracy:\", round(accuracy, 3) * 100, \"percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO #2: Make this section more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Varying Learning Rates by Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def setup_layers(hidden_neurons, outputs, learning_rate=1.0, learning_rate_layer_decay=1.0):\n",
    "    layers = []\n",
    "    for i in range(len(hidden_neurons)):\n",
    "        layer = FullyConnectedLR(neurons=hidden_neurons[i], activation_function=sigmoid)\n",
    "        setattr(layer, \"learning_rate\", learning_rate / (learning_rate_layer_decay ** i))\n",
    "        layers.append(layer)\n",
    "\n",
    "    output_layer = FullyConnectedLR(neurons=outputs, activation_function=sigmoid)\n",
    "    setattr(output_layer, \"learning_rate\", learning_rate)\n",
    "    layers.append(output_layer)\n",
    "    return layers   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkLRDecay(NeuralNetworkLR):\n",
    "    def __init__(self, hidden_neurons, outputs, loss_function, learning_rate, \n",
    "                 learning_rate_layer_decay):\n",
    "        NeuralNetworkLR.__init__(self, hidden_neurons, outputs, loss_function, learning_rate)\n",
    "        self.learning_rate_layer_decay = learning_rate_layer_decay\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        \n",
    "        if not self.layers_setup:\n",
    "            self.layers = setup_layers(self.hidden_neurons, \n",
    "                                       self.outputs, \n",
    "                                       self.learning_rate,\n",
    "                                       self.learning_rate_layer_decay)\n",
    "            self.layers_setup = True\n",
    "\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkLRDecay(\n",
    "        hidden_neurons=[75, 25],\n",
    "        outputs=10,\n",
    "        loss_function=mean_square_error, \n",
    "        learning_rate=0.3, \n",
    "        learning_rate_layer_decay=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    train_learning_rate_decay(nn_mnist_lr, X_train, Y_train, epochs=5, print_msg=True,\n",
    "                              learning_rate_anneal=2)\n",
    "    accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Improvement in accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning Rate Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "TODO #3: Add the math to this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def setup_layers(hidden_neurons, outputs, learning_rate=1.0, learning_rate_layer_decay=1.0, momentum=0.1):\n",
    "    layers = []\n",
    "    for i in range(len(hidden_neurons)):\n",
    "        layer = FullyConnectedLRMomentum(neurons=hidden_neurons[i], activation_function=sigmoid)\n",
    "        setattr(layer, \"learning_rate\", learning_rate / (learning_rate_layer_decay ** i))\n",
    "        setattr(layer, \"momentum\", momentum)\n",
    "        layers.append(layer)\n",
    "\n",
    "    output_layer = FullyConnectedLRMomentum(neurons=outputs, activation_function=sigmoid)\n",
    "    setattr(output_layer, \"learning_rate\", learning_rate)\n",
    "    setattr(output_layer, \"momentum\", momentum)\n",
    "    layers.append(output_layer)\n",
    "    return layers   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class FullyConnectedLRMomentum(FullyConnectedLR):\n",
    "    def __init__(self, neurons, activation_function):\n",
    "        FullyConnectedLR.__init__(self, neurons, activation_function)\n",
    "        self.first = True\n",
    "    \n",
    "    def bprop(self, layer_gradient):\n",
    "        \n",
    "        dAOdAI = self.activation_function(self.AI, bprop=True)\n",
    "        dLGdAI = layer_gradient * dAOdAI\n",
    "        dAIdW = self.LI.T\n",
    "        weight_update_current = np.dot(dAIdW, dLGdAI)\n",
    "        \n",
    "        if self.first:    \n",
    "            self.velocity = self.learning_rate * weight_update_current\n",
    "            self.first = False\n",
    "        else:\n",
    "            self.velocity = np.add(self.momentum * self.velocity, \n",
    "                                   self.learning_rate * weight_update_current)\n",
    "        self.W = self.W - self.velocity\n",
    "        \n",
    "        self.iterations += 1\n",
    "        \n",
    "        output_grad = np.dot(dLGdAI, self.W.T)\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkLRMomentum(NeuralNetworkLRDecay):\n",
    "    def __init__(self, hidden_neurons, outputs, loss_function, learning_rate, \n",
    "                 learning_rate_layer_decay, momentum):\n",
    "        NeuralNetworkLRDecay.__init__(self, hidden_neurons, outputs, loss_function, \n",
    "                                      learning_rate, learning_rate_layer_decay)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        \n",
    "        if not self.layers_setup:\n",
    "            self.layers = setup_layers(self.hidden_neurons, \n",
    "                                       self.outputs, \n",
    "                                       self.learning_rate,\n",
    "                                       self.learning_rate_layer_decay, \n",
    "                                       self.momentum)\n",
    "            self.layers_setup = True\n",
    "\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkLRMomentum(\n",
    "        hidden_neurons=[75, 25],\n",
    "        outputs=10,\n",
    "        loss_function=mean_square_error, \n",
    "        learning_rate=0.3, \n",
    "        learning_rate_layer_decay=4,\n",
    "        momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    train_learning_rate_decay(nn_mnist_lr, X_train, Y_train, epochs=5, print_msg=True,\n",
    "                              learning_rate_anneal=2)\n",
    "    accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO #4: Make this section more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkDropout(NeuralNetworkLRMomentum):\n",
    "    def __init__(self, hidden_neurons, outputs, loss_function, learning_rate, \n",
    "                 learning_rate_layer_decay, momentum, dropout):\n",
    "        NeuralNetworkLRMomentum.__init__(self, hidden_neurons, outputs, loss_function, \n",
    "                                      learning_rate, learning_rate_layer_decay, momentum)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        \n",
    "    def forwardpass(self, X, predict=False):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        \n",
    "        if not self.layers_setup:\n",
    "            self.layers = setup_layers(self.hidden_neurons, \n",
    "                                       self.outputs, \n",
    "                                       self.learning_rate,\n",
    "                                       self.learning_rate_layer_decay, \n",
    "                                       self.momentum)\n",
    "            self.layers_setup = True\n",
    "\n",
    "        X_next = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if self.dropout and not predict:\n",
    "                zero_indices = np.random.choice(range(layer.n_neurons), \n",
    "                                                size=int(layer.n_neurons * (1 - self.dropout)), \n",
    "                                                replace=False)\n",
    "                X_next[:, zero_indices] = 0.0\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkDropout(\n",
    "        hidden_neurons=[75, 25],\n",
    "        outputs=10,\n",
    "        loss_function=mean_square_error, \n",
    "        learning_rate=0.3, \n",
    "        learning_rate_layer_decay=4,\n",
    "        momentum=0.5,\n",
    "        dropout=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    train_learning_rate_decay(nn_mnist_lr, X_train, Y_train, epochs=2, print_msg=True,\n",
    "                              learning_rate_anneal=2)\n",
    "    accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedXavier(FullyConnectedLRMomentum):\n",
    "    def __init__(self, neurons, activation_function):\n",
    "        FullyConnectedLR.__init__(self, neurons, activation_function)\n",
    "        self.first = True\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.LI = layer_input\n",
    "        \n",
    "        if not self.weights_initialized:\n",
    "            self.W = np.random.normal(scale=1 / np.sqrt(self.n_neurons), size=(self.LI.shape[1], self.n_neurons))\n",
    "            self.weights_initialized = True\n",
    "        \n",
    "        self.AI = np.dot(self.LI, self.W)\n",
    "        return self.activation_function(self.AI, bprop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_layers(hidden_neurons, outputs, learning_rate=1.0, learning_rate_layer_decay=1.0, momentum=0.1):\n",
    "    layers = []\n",
    "    for i in range(len(hidden_neurons)):\n",
    "        layer = FullyConnectedXavier(neurons=hidden_neurons[i], activation_function=sigmoid)\n",
    "        setattr(layer, \"learning_rate\", learning_rate / (learning_rate_layer_decay ** i))\n",
    "        setattr(layer, \"momentum\", momentum)\n",
    "        layers.append(layer)\n",
    "\n",
    "    output_layer = FullyConnectedXavier(neurons=outputs, activation_function=sigmoid)\n",
    "    setattr(output_layer, \"learning_rate\", learning_rate)\n",
    "    setattr(output_layer, \"momentum\", momentum)\n",
    "    layers.append(output_layer)\n",
    "    return layers   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkDropout(\n",
    "        hidden_neurons=[75, 25],\n",
    "        outputs=10,\n",
    "        loss_function=mean_square_error, \n",
    "        learning_rate=0.3, \n",
    "        learning_rate_layer_decay=4,\n",
    "        momentum=0.5,\n",
    "        dropout=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    train_learning_rate_decay(nn_mnist_lr, X_train, Y_train, epochs=2, print_msg=True,\n",
    "                              learning_rate_anneal=2)\n",
    "    accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO #5: Make this section more readable, RISE compatible, consider moving it up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, bprop=False):\n",
    "    if bprop:\n",
    "        return sigmoid(x) * (1-sigmoid(x))\n",
    "    else:\n",
    "        return 1.0/(1.0+np.exp(-cap_sigmoid_input(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(prediction, Y, bprop=False):\n",
    "    if bprop:\n",
    "        return -1.0 * Y / prediction + (1.0 - Y) / (1.0 - prediction)\n",
    "    else:\n",
    "        return -1.0 * Y * np.log(prediction) - (1.0 - Y) * (np.log(1.0 - prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkActivation(\n",
    "        hidden_neurons=[50],\n",
    "        outputs=10,\n",
    "        loss_function=cross_entropy, \n",
    "        learning_rate=0.1, \n",
    "        learning_rate_layer_decay=1,\n",
    "        momentum=0.1,\n",
    "        dropout=0.8, \n",
    "        activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate on epoch 1 is 0.1\n",
      "> <ipython-input-151-ab2cf08698be>(6)cross_entropy()\n",
      "-> return -1.0 * Y * np.log(prediction) - (1.0 - Y) * (np.log(1.0 - prediction))\n",
      "(Pdb) \n",
      "(Pdb) prediction\n",
      "array([[ 0.07422254,  0.82941091,  0.50139775,  0.4122644 ,  0.23976789,\n",
      "         0.77784322,  0.15406458,  0.78788671,  0.75771713,  0.37591111]])\n",
      "(Pdb) Y\n",
      "array([[ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]])\n",
      "(Pdb) exit()\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-543032688265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_learning_rate_decay(nn_mnist_lr, X_train, Y_train, epochs=1, print_msg=True,\n\u001b[0;32m----> 2\u001b[0;31m                           learning_rate_anneal=1)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_mnist_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-7f7044e596c0>\u001b[0m in \u001b[0;36mtrain_learning_rate_decay\u001b[0;34m(net, X_train, Y_train, epochs, print_msg, learning_rate_anneal)\u001b[0m\n\u001b[1;32m     11\u001b[0m             print(\"Learning rate on epoch\", i+1, \n\u001b[1;32m     12\u001b[0m                   \"is\", net.learning_rate)\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mone_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprint_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done with epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/ODSC_Neural_Nets_11-04-17/neural_net.py\u001b[0m in \u001b[0;36mone_epoch\u001b[0;34m(net, X, Y)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mneural_net_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/ODSC_Neural_Nets_11-04-17/neural_net.py\u001b[0m in \u001b[0;36mneural_net_pass\u001b[0;34m(net, x, y)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mneural_net_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropogate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-cb12c4b29dd6>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, prediction, Y)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;34m\"\"\" Calculate the loss on the data and send the result backwards through the net. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbprop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-151-ab2cf08698be>\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(prediction, Y, bprop)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m \u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-151-ab2cf08698be>\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(prediction, Y, bprop)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m \u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_learning_rate_decay(nn_mnist_lr, X_train, Y_train, epochs=1, print_msg=True,\n",
    "                          learning_rate_anneal=1)\n",
    "accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test, predict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkActivation(\n",
    "        hidden_neurons=[50],\n",
    "        outputs=10,\n",
    "        loss_function=mean_square_error, \n",
    "        learning_rate=0.1, \n",
    "        learning_rate_layer_decay=1,\n",
    "        momentum=0.1,\n",
    "        dropout=0.8, \n",
    "        activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate on epoch 1 is 0.1\n",
      "Done with epoch 1\n",
      "Neural Net MNIST Classification Accuracy: 93.2 percent\n"
     ]
    }
   ],
   "source": [
    "train_learning_rate_decay(nn_mnist_lr, X_train, Y_train, epochs=1, print_msg=True,\n",
    "                          learning_rate_anneal=1)\n",
    "accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test, predict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkActivation(\n",
    "        hidden_neurons=[50],\n",
    "        outputs=10,\n",
    "        loss_function=cross_entropy, \n",
    "        learning_rate=0.1, \n",
    "        learning_rate_layer_decay=1,\n",
    "        momentum=0.1,\n",
    "        dropout=0, \n",
    "        activation_function=leaky_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate on epoch 1 is 0.1\n"
     ]
    },
    {
     "ename": "RuntimeWarning",
     "evalue": "divide by zero encountered in log",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeWarning\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-216-543032688265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_learning_rate_decay(nn_mnist_lr, X_train, Y_train, epochs=1, print_msg=True,\n\u001b[0;32m----> 2\u001b[0;31m                           learning_rate_anneal=1)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_mnist_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-7f7044e596c0>\u001b[0m in \u001b[0;36mtrain_learning_rate_decay\u001b[0;34m(net, X_train, Y_train, epochs, print_msg, learning_rate_anneal)\u001b[0m\n\u001b[1;32m     11\u001b[0m             print(\"Learning rate on epoch\", i+1, \n\u001b[1;32m     12\u001b[0m                   \"is\", net.learning_rate)\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mone_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprint_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done with epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/ODSC_Neural_Nets_11-04-17/neural_net.py\u001b[0m in \u001b[0;36mone_epoch\u001b[0;34m(net, X, Y)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mneural_net_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/ODSC_Neural_Nets_11-04-17/neural_net.py\u001b[0m in \u001b[0;36mneural_net_pass\u001b[0;34m(net, x, y)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mneural_net_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropogate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-cb12c4b29dd6>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, prediction, Y)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;34m\"\"\" Calculate the loss on the data and send the result backwards through the net. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbprop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-154-e8f141fb7b88>\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(prediction, Y, bprop)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeWarning\u001b[0m: divide by zero encountered in log"
     ]
    }
   ],
   "source": [
    "train_learning_rate_decay(nn_mnist_lr, X_train, Y_train, epochs=1, print_msg=True,\n",
    "                          learning_rate_anneal=1)\n",
    "accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test, predict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO #6: Make this last section more readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_drop_connect_weights(weights, drop_connect):\n",
    "    new_weights = weights.copy()\n",
    "    num_weights = new_weights.shape[0] * new_weights.shape[1]\n",
    "    reshaped_weights = np.reshape(new_weights, (num_weights, 1))\n",
    "    zero_indices = np.random.choice(range(num_weights), \n",
    "                                    size=int(num_weights * (1 - drop_connect)), \n",
    "                                    replace=False)\n",
    "    reshaped_weights[zero_indices, :] = 0.0\n",
    "    drop_connected_weights = np.reshape(reshaped_weights, new_weights.shape)\n",
    "    \n",
    "    return drop_connected_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedDropConnect(FullyConnectedLRMomentum):\n",
    "    \n",
    "    def fprop(self, layer_input):\n",
    "        \n",
    "        if self.drop_connect:            \n",
    "            drop_connected_weights = apply_drop_connect_weights(self.W, \n",
    "                                                                self.drop_connect)\n",
    "            self.activation_input = np.dot(layer_input, \n",
    "                                           drop_connected_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Different Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.01, bprop=False):\n",
    "    if bprop:\n",
    "        dx = np.full(x.shape, alpha)\n",
    "        dx[x >= 0] = 1\n",
    "        return dx\n",
    "    else:\n",
    "        return np.maximum(alpha * x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, bprop=False):\n",
    "    if bprop:\n",
    "        e = np.exp(2*cap_sigmoid_input(x))\n",
    "        return (e-1)/(e+1)\n",
    "    else:\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_sigmoid_input(a):\n",
    "    a[a < -100] = -100\n",
    "    a[a > 100] = 100\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def setup_layers(hidden_neurons, outputs, \n",
    "                 learning_rate=1.0, \n",
    "                 learning_rate_layer_decay=1.0, \n",
    "                 momentum=0.1,\n",
    "                 activation_function=sigmoid):\n",
    "    layers = []\n",
    "    for i in range(len(hidden_neurons)):\n",
    "        layer = FullyConnectedXavier(neurons=hidden_neurons[i], activation_function=activation_function)\n",
    "        setattr(layer, \"learning_rate\", learning_rate / (learning_rate_layer_decay ** i))\n",
    "        setattr(layer, \"momentum\", momentum)\n",
    "        layers.append(layer)\n",
    "\n",
    "    output_layer = FullyConnectedXavier(neurons=outputs, activation_function=sigmoid)\n",
    "    setattr(output_layer, \"learning_rate\", learning_rate / (learning_rate_layer_decay ** (len(hidden_neurons) + 1)))\n",
    "    setattr(output_layer, \"momentum\", momentum)\n",
    "    layers.append(output_layer)\n",
    "    return layers   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkActivation(NeuralNetworkDropout):\n",
    "    def __init__(self, hidden_neurons, outputs, loss_function, learning_rate, \n",
    "                 learning_rate_layer_decay, momentum, dropout, activation_function):\n",
    "        NeuralNetworkDropout.__init__(self, hidden_neurons, outputs, loss_function, \n",
    "                                      learning_rate, learning_rate_layer_decay, momentum, dropout)\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "        \n",
    "    def forwardpass(self, X, predict=False):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        \n",
    "        if not self.layers_setup:\n",
    "            self.layers = setup_layers(self.hidden_neurons, \n",
    "                                       self.outputs, \n",
    "                                       self.learning_rate,\n",
    "                                       self.learning_rate_layer_decay, \n",
    "                                       self.momentum, \n",
    "                                       self.activation_function)\n",
    "            self.layers_setup = True\n",
    "\n",
    "        X_next = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if self.dropout and not predict:\n",
    "                zero_indices = np.random.choice(range(layer.n_neurons), \n",
    "                                                size=int(layer.n_neurons * (1 - self.dropout)), \n",
    "                                                replace=False)\n",
    "                if X_next is None:\n",
    "                    import pdb; pdb.set_trace()\n",
    "                X_next[:, zero_indices] = 0.0\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkActivation(\n",
    "        hidden_neurons=[50],\n",
    "        outputs=10,\n",
    "        loss_function=mean_square_error, \n",
    "        learning_rate=0.1, \n",
    "        learning_rate_layer_decay=1,\n",
    "        momentum=0.1,\n",
    "        dropout=0.8, \n",
    "        activation_function=leaky_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate on epoch 1 is 0.1\n",
      "Done with epoch 1\n",
      "Neural Net MNIST Classification Accuracy: 36.2 percent\n"
     ]
    }
   ],
   "source": [
    "train_learning_rate_decay(nn_mnist_lr, X_train, Y_train, epochs=1, print_msg=True,\n",
    "                          learning_rate_anneal=1)\n",
    "accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test, predict=True)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "47px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
